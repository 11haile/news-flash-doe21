* 
* ==> Audit <==
* |-----------|---------------------------------|----------|-----------|---------|---------------------|---------------------|
|  Command  |              Args               | Profile  |   User    | Version |     Start Time      |      End Time       |
|-----------|---------------------------------|----------|-----------|---------|---------------------|---------------------|
| start     |                                 | minikube | sem.haile | v1.29.0 | 05 Mar 23 01:16 CET | 05 Mar 23 01:18 CET |
| kubectl   | -- get po -A                    | minikube | sem.haile | v1.29.0 | 05 Mar 23 01:22 CET | 05 Mar 23 01:22 CET |
| dashboard |                                 | minikube | sem.haile | v1.29.0 | 05 Mar 23 01:22 CET |                     |
| kubectl   | -- create deployment            | minikube | sem.haile | v1.29.0 | 05 Mar 23 01:29 CET | 05 Mar 23 01:29 CET |
|           | hello-minikube                  |          |           |         |                     |                     |
|           | --image=kicbase/echo-server:1.0 |          |           |         |                     |                     |
| kubectl   | -- expose deployment            | minikube | sem.haile | v1.29.0 | 05 Mar 23 01:29 CET | 05 Mar 23 01:29 CET |
|           | hello-minikube --type=NodePort  |          |           |         |                     |                     |
|           | --port=8080                     |          |           |         |                     |                     |
| kubectl   | -- get services hello-minikube  | minikube | sem.haile | v1.29.0 | 05 Mar 23 01:29 CET | 05 Mar 23 01:29 CET |
| service   | hello-minikube                  | minikube | sem.haile | v1.29.0 | 05 Mar 23 01:29 CET |                     |
| start     | --network=socket_vmnet          | minikube | sem.haile | v1.29.0 | 05 Mar 23 01:30 CET |                     |
| start     | --network=socket_vmnet          | minikube | sem.haile | v1.29.0 | 05 Mar 23 16:00 CET |                     |
| start     | --network=socket_vmnet          | minikube | sem.haile | v1.29.0 | 05 Mar 23 16:09 CET |                     |
| start     |                                 | minikube | sem.haile | v1.29.0 | 06 Mar 23 10:12 CET |                     |
| start     |                                 | minikube | sem.haile | v1.29.0 | 06 Mar 23 10:44 CET |                     |
| start     | --network=socket_vmnet          | minikube | sem.haile | v1.29.0 | 06 Mar 23 10:58 CET |                     |
| start     | --network=socket_vmnet          | minikube | sem.haile | v1.29.0 | 06 Mar 23 14:40 CET |                     |
|-----------|---------------------------------|----------|-----------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/03/06 14:40:51
Running on machine: bnmc02c73ublvdl
Binary: Built with gc go1.19.5 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0306 14:40:51.157330   47093 out.go:296] Setting OutFile to fd 1 ...
I0306 14:40:51.157630   47093 out.go:348] isatty.IsTerminal(1) = true
I0306 14:40:51.157633   47093 out.go:309] Setting ErrFile to fd 2...
I0306 14:40:51.157637   47093 out.go:348] isatty.IsTerminal(2) = true
I0306 14:40:51.157735   47093 root.go:334] Updating PATH: /Users/sem.haile/.minikube/bin
W0306 14:40:51.157839   47093 root.go:311] Error reading config file at /Users/sem.haile/.minikube/config/config.json: open /Users/sem.haile/.minikube/config/config.json: no such file or directory
I0306 14:40:51.159781   47093 out.go:303] Setting JSON to false
I0306 14:40:51.221853   47093 start.go:125] hostinfo: {"hostname":"bnmc02c73ublvdl","uptime":74868,"bootTime":1678035183,"procs":737,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"13.2.1","kernelVersion":"22.3.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"d514dfee-1671-52b3-8549-064aa724735a"}
W0306 14:40:51.222014   47093 start.go:133] gopshost.Virtualization returned error: not implemented yet
I0306 14:40:51.246426   47093 out.go:177] üòÑ  minikube v1.29.0 on Darwin 13.2.1
I0306 14:40:51.267510   47093 notify.go:220] Checking for updates...
I0306 14:40:51.267885   47093 config.go:180] Loaded profile config "minikube": Driver=qemu2, ContainerRuntime=docker, KubernetesVersion=v1.26.1
I0306 14:40:51.267955   47093 driver.go:365] Setting default libvirt URI to qemu:///system
I0306 14:40:51.291458   47093 out.go:177] ‚ú®  Using the qemu2 driver based on existing profile
I0306 14:40:51.333608   47093 start.go:296] selected driver: qemu2
I0306 14:40:51.333620   47093 start.go:857] validating driver "qemu2" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.29.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.37@sha256:8bf7a0e8a062bc5e2b71d28b35bfa9cc862d9220e234e86176b3785f685d8b15 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:qemu2 HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:51379 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.26.1 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:10.0.2.15 Port:8443 KubernetesVersion:v1.26.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network:socket_vmnet Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP:}
I0306 14:40:51.333728   47093 start.go:868] status for qemu2: {Installed:true Healthy:true Running:true NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0306 14:40:51.333950   47093 cni.go:84] Creating CNI manager for ""
I0306 14:40:51.333961   47093 cni.go:157] "qemu2" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0306 14:40:51.333980   47093 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.29.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.37@sha256:8bf7a0e8a062bc5e2b71d28b35bfa9cc862d9220e234e86176b3785f685d8b15 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:qemu2 HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:51379 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.26.1 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:10.0.2.15 Port:8443 KubernetesVersion:v1.26.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network:socket_vmnet Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP:}
I0306 14:40:51.334112   47093 iso.go:125] acquiring lock: {Name:mk99a888a650315f49b53a4cf7ff849b0f73f164 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0306 14:40:51.354345   47093 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0306 14:40:51.394323   47093 preload.go:132] Checking if preload exists for k8s version v1.26.1 and runtime docker
I0306 14:40:51.394393   47093 preload.go:148] Found local preload: /Users/sem.haile/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.26.1-docker-overlay2-amd64.tar.lz4
I0306 14:40:51.394410   47093 cache.go:57] Caching tarball of preloaded images
I0306 14:40:51.394576   47093 preload.go:174] Found /Users/sem.haile/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.26.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0306 14:40:51.394596   47093 cache.go:60] Finished verifying existence of preloaded tar for  v1.26.1 on docker
I0306 14:40:51.394721   47093 profile.go:148] Saving config to /Users/sem.haile/.minikube/profiles/minikube/config.json ...
I0306 14:40:51.395486   47093 cache.go:193] Successfully downloaded all kic artifacts
I0306 14:40:51.395522   47093 start.go:364] acquiring machines lock for minikube: {Name:mk16e1714e4d0f95e2b0e4f01ec9b813973f990a Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I0306 14:40:51.395675   47093 start.go:368] acquired machines lock for "minikube" in 138.211¬µs
I0306 14:40:51.395704   47093 start.go:96] Skipping create...Using existing machine configuration
I0306 14:40:51.395714   47093 fix.go:55] fixHost starting: 
I0306 14:40:51.397825   47093 fix.go:103] recreateIfNeeded on minikube: state=Running err=<nil>
W0306 14:40:51.397835   47093 fix.go:129] unexpected machine state, will restart: <nil>
I0306 14:40:51.438234   47093 out.go:177] üèÉ  Updating the running qemu2 "minikube" VM ...
I0306 14:40:51.458123   47093 machine.go:88] provisioning docker machine ...
I0306 14:40:51.458155   47093 buildroot.go:166] provisioning hostname "minikube"
I0306 14:40:51.458302   47093 main.go:141] libmachine: Using SSH client type: native
I0306 14:40:51.458528   47093 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f3280] 0x1003f6400 <nil>  [] 0s} localhost 51347 <nil> <nil>}
I0306 14:40:51.458535   47093 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0306 14:40:51.575663   47093 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0306 14:40:51.575768   47093 main.go:141] libmachine: Using SSH client type: native
I0306 14:40:51.575926   47093 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f3280] 0x1003f6400 <nil>  [] 0s} localhost 51347 <nil> <nil>}
I0306 14:40:51.575935   47093 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0306 14:40:51.655154   47093 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0306 14:40:51.655173   47093 buildroot.go:172] set auth options {CertDir:/Users/sem.haile/.minikube CaCertPath:/Users/sem.haile/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/sem.haile/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/sem.haile/.minikube/machines/server.pem ServerKeyPath:/Users/sem.haile/.minikube/machines/server-key.pem ClientKeyPath:/Users/sem.haile/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/sem.haile/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/sem.haile/.minikube}
I0306 14:40:51.655195   47093 buildroot.go:174] setting up certificates
I0306 14:40:51.655207   47093 provision.go:83] configureAuth start
I0306 14:40:51.655211   47093 provision.go:138] copyHostCerts
I0306 14:40:51.655339   47093 exec_runner.go:144] found /Users/sem.haile/.minikube/key.pem, removing ...
I0306 14:40:51.655346   47093 exec_runner.go:207] rm: /Users/sem.haile/.minikube/key.pem
I0306 14:40:51.656202   47093 exec_runner.go:151] cp: /Users/sem.haile/.minikube/certs/key.pem --> /Users/sem.haile/.minikube/key.pem (1679 bytes)
I0306 14:40:51.656752   47093 exec_runner.go:144] found /Users/sem.haile/.minikube/ca.pem, removing ...
I0306 14:40:51.656756   47093 exec_runner.go:207] rm: /Users/sem.haile/.minikube/ca.pem
I0306 14:40:51.656857   47093 exec_runner.go:151] cp: /Users/sem.haile/.minikube/certs/ca.pem --> /Users/sem.haile/.minikube/ca.pem (1086 bytes)
I0306 14:40:51.657368   47093 exec_runner.go:144] found /Users/sem.haile/.minikube/cert.pem, removing ...
I0306 14:40:51.657379   47093 exec_runner.go:207] rm: /Users/sem.haile/.minikube/cert.pem
I0306 14:40:51.657529   47093 exec_runner.go:151] cp: /Users/sem.haile/.minikube/certs/cert.pem --> /Users/sem.haile/.minikube/cert.pem (1131 bytes)
I0306 14:40:51.657919   47093 provision.go:112] generating server cert: /Users/sem.haile/.minikube/machines/server.pem ca-key=/Users/sem.haile/.minikube/certs/ca.pem private-key=/Users/sem.haile/.minikube/certs/ca-key.pem org=sem.haile.minikube san=[127.0.0.1 localhost localhost 127.0.0.1 minikube minikube]
I0306 14:40:51.805801   47093 provision.go:172] copyRemoteCerts
I0306 14:40:51.805941   47093 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0306 14:40:51.805957   47093 sshutil.go:53] new ssh client: &{IP:localhost Port:51347 SSHKeyPath:/Users/sem.haile/.minikube/machines/minikube/id_rsa Username:docker}
I0306 14:40:51.867565   47093 ssh_runner.go:362] scp /Users/sem.haile/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1086 bytes)
I0306 14:40:51.986791   47093 ssh_runner.go:362] scp /Users/sem.haile/.minikube/machines/server.pem --> /etc/docker/server.pem (1216 bytes)
I0306 14:40:52.093397   47093 ssh_runner.go:362] scp /Users/sem.haile/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0306 14:40:52.196921   47093 provision.go:86] duration metric: configureAuth took 541.696514ms
I0306 14:40:52.196932   47093 buildroot.go:189] setting minikube options for container-runtime
I0306 14:40:52.197105   47093 config.go:180] Loaded profile config "minikube": Driver=qemu2, ContainerRuntime=docker, KubernetesVersion=v1.26.1
I0306 14:40:52.197178   47093 main.go:141] libmachine: Using SSH client type: native
I0306 14:40:52.197304   47093 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f3280] 0x1003f6400 <nil>  [] 0s} localhost 51347 <nil> <nil>}
I0306 14:40:52.197312   47093 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0306 14:40:52.290413   47093 main.go:141] libmachine: SSH cmd err, output: <nil>: tmpfs

I0306 14:40:52.290427   47093 buildroot.go:70] root file system type: tmpfs
I0306 14:40:52.290576   47093 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0306 14:40:52.290680   47093 main.go:141] libmachine: Using SSH client type: native
I0306 14:40:52.290838   47093 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f3280] 0x1003f6400 <nil>  [] 0s} localhost 51347 <nil> <nil>}
I0306 14:40:52.290883   47093 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=qemu2 --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0306 14:40:52.426169   47093 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=qemu2 --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0306 14:40:52.426259   47093 main.go:141] libmachine: Using SSH client type: native
I0306 14:40:52.426385   47093 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f3280] 0x1003f6400 <nil>  [] 0s} localhost 51347 <nil> <nil>}
I0306 14:40:52.426394   47093 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0306 14:40:52.536365   47093 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0306 14:40:52.536375   47093 machine.go:91] provisioned docker machine in 1.078239145s
I0306 14:40:52.536387   47093 start.go:300] post-start starting for "minikube" (driver="qemu2")
I0306 14:40:52.536392   47093 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0306 14:40:52.536484   47093 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0306 14:40:52.536494   47093 sshutil.go:53] new ssh client: &{IP:localhost Port:51347 SSHKeyPath:/Users/sem.haile/.minikube/machines/minikube/id_rsa Username:docker}
I0306 14:40:52.610851   47093 ssh_runner.go:195] Run: cat /etc/os-release
I0306 14:40:52.616833   47093 info.go:137] Remote host: Buildroot 2021.02.12
I0306 14:40:52.616848   47093 filesync.go:126] Scanning /Users/sem.haile/.minikube/addons for local assets ...
I0306 14:40:52.616997   47093 filesync.go:126] Scanning /Users/sem.haile/.minikube/files for local assets ...
I0306 14:40:52.617068   47093 start.go:303] post-start completed in 80.676129ms
I0306 14:40:52.617077   47093 fix.go:57] fixHost completed within 1.22136051s
I0306 14:40:52.617129   47093 main.go:141] libmachine: Using SSH client type: native
I0306 14:40:52.617260   47093 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1003f3280] 0x1003f6400 <nil>  [] 0s} localhost 51347 <nil> <nil>}
I0306 14:40:52.617265   47093 main.go:141] libmachine: About to run SSH command:
date +%!s(MISSING).%!N(MISSING)
I0306 14:40:52.717687   47093 main.go:141] libmachine: SSH cmd err, output: <nil>: 1678110052.913577069

I0306 14:40:52.717700   47093 fix.go:207] guest clock: 1678110052.913577069
I0306 14:40:52.717708   47093 fix.go:220] Guest: 2023-03-06 14:40:52.913577069 +0100 CET Remote: 2023-03-06 14:40:52.617078 +0100 CET m=+1.545174577 (delta=296.499069ms)
I0306 14:40:52.717722   47093 fix.go:191] guest clock delta is within tolerance: 296.499069ms
I0306 14:40:52.717725   47093 start.go:83] releasing machines lock for "minikube", held for 1.32203713s
I0306 14:40:52.717866   47093 ssh_runner.go:195] Run: cat /version.json
I0306 14:40:52.717875   47093 sshutil.go:53] new ssh client: &{IP:localhost Port:51347 SSHKeyPath:/Users/sem.haile/.minikube/machines/minikube/id_rsa Username:docker}
I0306 14:40:52.718621   47093 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0306 14:40:52.718649   47093 sshutil.go:53] new ssh client: &{IP:localhost Port:51347 SSHKeyPath:/Users/sem.haile/.minikube/machines/minikube/id_rsa Username:docker}
I0306 14:40:52.790263   47093 ssh_runner.go:195] Run: systemctl --version
I0306 14:40:52.864441   47093 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W0306 14:40:52.876058   47093 cni.go:208] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0306 14:40:52.876169   47093 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0306 14:40:52.897059   47093 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (135 bytes)
I0306 14:40:52.936485   47093 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0306 14:40:52.956767   47093 cni.go:258] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0306 14:40:52.956779   47093 preload.go:132] Checking if preload exists for k8s version v1.26.1 and runtime docker
I0306 14:40:52.956884   47093 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0306 14:40:52.996805   47093 docker.go:630] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.26.1
registry.k8s.io/kube-controller-manager:v1.26.1
registry.k8s.io/kube-scheduler:v1.26.1
registry.k8s.io/kube-proxy:v1.26.1
registry.k8s.io/etcd:3.5.6-0
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kicbase/echo-server:1.0
kubernetesui/metrics-scraper:<none>
registry.k8s.io/coredns/coredns:v1.9.3
registry.k8s.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0306 14:40:52.996817   47093 docker.go:560] Images already preloaded, skipping extraction
I0306 14:40:52.996824   47093 start.go:483] detecting cgroup driver to use...
I0306 14:40:52.996919   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0306 14:40:53.053634   47093 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0306 14:40:53.081068   47093 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0306 14:40:53.105989   47093 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0306 14:40:53.106072   47093 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0306 14:40:53.125257   47093 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0306 14:40:53.161393   47093 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0306 14:40:53.190900   47093 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0306 14:40:53.240820   47093 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0306 14:40:53.271943   47093 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0306 14:40:53.300368   47093 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0306 14:40:53.322861   47093 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0306 14:40:53.348103   47093 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0306 14:40:53.572879   47093 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0306 14:40:53.619640   47093 start.go:483] detecting cgroup driver to use...
I0306 14:40:53.619826   47093 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0306 14:40:53.681816   47093 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0306 14:40:53.729874   47093 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I0306 14:40:53.804183   47093 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0306 14:40:53.843086   47093 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0306 14:40:53.871983   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0306 14:40:53.937021   47093 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0306 14:40:54.198306   47093 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0306 14:40:54.465769   47093 docker.go:529] configuring docker to use "cgroupfs" as cgroup driver...
I0306 14:40:54.465782   47093 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (144 bytes)
I0306 14:40:54.517497   47093 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0306 14:40:54.780576   47093 ssh_runner.go:195] Run: sudo systemctl restart docker
I0306 14:41:08.034672   47093 ssh_runner.go:235] Completed: sudo systemctl restart docker: (13.254007134s)
I0306 14:41:08.034781   47093 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0306 14:41:08.340926   47093 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0306 14:41:08.786626   47093 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0306 14:41:09.193525   47093 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0306 14:41:09.558694   47093 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0306 14:41:09.679219   47093 start.go:530] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0306 14:41:09.679355   47093 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0306 14:41:09.715510   47093 start.go:551] Will wait 60s for crictl version
I0306 14:41:09.715722   47093 ssh_runner.go:195] Run: which crictl
I0306 14:41:09.729010   47093 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0306 14:41:10.463711   47093 start.go:567] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.23
RuntimeApiVersion:  v1alpha2
I0306 14:41:10.463811   47093 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0306 14:41:10.656678   47093 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0306 14:41:11.025123   47093 out.go:204] üê≥  Preparing Kubernetes v1.26.1 on Docker 20.10.23 ...
I0306 14:41:11.025342   47093 ssh_runner.go:195] Run: grep 10.0.2.2	host.minikube.internal$ /etc/hosts
I0306 14:41:11.035910   47093 preload.go:132] Checking if preload exists for k8s version v1.26.1 and runtime docker
I0306 14:41:11.035976   47093 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0306 14:41:11.172820   47093 docker.go:630] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.26.1
registry.k8s.io/kube-controller-manager:v1.26.1
registry.k8s.io/kube-scheduler:v1.26.1
registry.k8s.io/kube-proxy:v1.26.1
registry.k8s.io/etcd:3.5.6-0
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kicbase/echo-server:1.0
kubernetesui/metrics-scraper:<none>
registry.k8s.io/coredns/coredns:v1.9.3
registry.k8s.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0306 14:41:11.179402   47093 docker.go:560] Images already preloaded, skipping extraction
I0306 14:41:11.179504   47093 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0306 14:41:11.280150   47093 docker.go:630] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.26.1
registry.k8s.io/kube-controller-manager:v1.26.1
registry.k8s.io/kube-scheduler:v1.26.1
registry.k8s.io/kube-proxy:v1.26.1
registry.k8s.io/etcd:3.5.6-0
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kicbase/echo-server:1.0
kubernetesui/metrics-scraper:<none>
registry.k8s.io/coredns/coredns:v1.9.3
registry.k8s.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0306 14:41:11.280164   47093 cache_images.go:84] Images are preloaded, skipping loading
I0306 14:41:11.280260   47093 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0306 14:41:11.448210   47093 cni.go:84] Creating CNI manager for ""
I0306 14:41:11.448223   47093 cni.go:157] "qemu2" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0306 14:41:11.448241   47093 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0306 14:41:11.448259   47093 kubeadm.go:172] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:10.0.2.15 APIServerPort:8443 KubernetesVersion:v1.26.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "10.0.2.15"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:10.0.2.15 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m]}
I0306 14:41:11.448366   47093 kubeadm.go:177] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.2.15
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 10.0.2.15
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "10.0.2.15"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.26.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0306 14:41:11.448431   47093 kubeadm.go:968] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.26.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=10.0.2.15

[Install]
 config:
{KubernetesVersion:v1.26.1 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0306 14:41:11.448533   47093 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.26.1
I0306 14:41:11.490827   47093 binaries.go:44] Found k8s binaries, skipping transfer
I0306 14:41:11.490930   47093 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0306 14:41:11.529308   47093 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (437 bytes)
I0306 14:41:11.619567   47093 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0306 14:41:11.803877   47093 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2075 bytes)
I0306 14:41:11.984855   47093 ssh_runner.go:195] Run: grep 10.0.2.15	control-plane.minikube.internal$ /etc/hosts
I0306 14:41:12.032975   47093 certs.go:56] Setting up /Users/sem.haile/.minikube/profiles/minikube for IP: 10.0.2.15
I0306 14:41:12.032995   47093 certs.go:186] acquiring lock for shared ca certs: {Name:mkba44a173b10bf4648a529933173bc0c2cc66be Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0306 14:41:12.033994   47093 certs.go:195] skipping minikubeCA CA generation: /Users/sem.haile/.minikube/ca.key
I0306 14:41:12.034561   47093 certs.go:195] skipping proxyClientCA CA generation: /Users/sem.haile/.minikube/proxy-client-ca.key
I0306 14:41:12.034994   47093 certs.go:311] skipping minikube-user signed cert generation: /Users/sem.haile/.minikube/profiles/minikube/client.key
I0306 14:41:12.035311   47093 certs.go:311] skipping minikube signed cert generation: /Users/sem.haile/.minikube/profiles/minikube/apiserver.key.49504c3e
I0306 14:41:12.036061   47093 certs.go:311] skipping aggregator signed cert generation: /Users/sem.haile/.minikube/profiles/minikube/proxy-client.key
I0306 14:41:12.036504   47093 certs.go:401] found cert: /Users/sem.haile/.minikube/certs/Users/sem.haile/.minikube/certs/ca-key.pem (1675 bytes)
I0306 14:41:12.036568   47093 certs.go:401] found cert: /Users/sem.haile/.minikube/certs/Users/sem.haile/.minikube/certs/ca.pem (1086 bytes)
I0306 14:41:12.036627   47093 certs.go:401] found cert: /Users/sem.haile/.minikube/certs/Users/sem.haile/.minikube/certs/cert.pem (1131 bytes)
I0306 14:41:12.036689   47093 certs.go:401] found cert: /Users/sem.haile/.minikube/certs/Users/sem.haile/.minikube/certs/key.pem (1679 bytes)
I0306 14:41:12.037302   47093 ssh_runner.go:362] scp /Users/sem.haile/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0306 14:41:12.476314   47093 ssh_runner.go:362] scp /Users/sem.haile/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0306 14:41:12.798967   47093 ssh_runner.go:362] scp /Users/sem.haile/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0306 14:41:13.060968   47093 ssh_runner.go:362] scp /Users/sem.haile/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0306 14:41:13.293325   47093 ssh_runner.go:362] scp /Users/sem.haile/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0306 14:41:13.498814   47093 ssh_runner.go:362] scp /Users/sem.haile/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0306 14:41:13.627178   47093 ssh_runner.go:362] scp /Users/sem.haile/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0306 14:41:13.756490   47093 ssh_runner.go:362] scp /Users/sem.haile/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0306 14:41:13.940152   47093 ssh_runner.go:362] scp /Users/sem.haile/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0306 14:41:14.075135   47093 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0306 14:41:14.154524   47093 ssh_runner.go:195] Run: openssl version
I0306 14:41:14.172430   47093 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0306 14:41:14.220281   47093 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0306 14:41:14.233195   47093 certs.go:444] hashing: -rw-r--r-- 1 root root 1111 Mar  5 00:18 /usr/share/ca-certificates/minikubeCA.pem
I0306 14:41:14.233298   47093 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0306 14:41:14.248735   47093 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0306 14:41:14.421909   47093 kubeadm.go:401] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.29.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.37@sha256:8bf7a0e8a062bc5e2b71d28b35bfa9cc862d9220e234e86176b3785f685d8b15 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:qemu2 HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:51379 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.26.1 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:10.0.2.15 Port:8443 KubernetesVersion:v1.26.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network:socket_vmnet Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP:}
I0306 14:41:14.422087   47093 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0306 14:41:14.523713   47093 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0306 14:41:14.846144   47093 host.go:66] Checking if "minikube" exists ...
I0306 14:41:14.847770   47093 main.go:141] libmachine: Using SSH client type: external
I0306 14:41:14.847806   47093 main.go:141] libmachine: Using SSH private key: /Users/sem.haile/.minikube/machines/minikube/id_rsa (-rw-------)
I0306 14:41:14.847831   47093 main.go:141] libmachine: &{[-F /dev/null -o ConnectionAttempts=3 -o ConnectTimeout=10 -o ControlMaster=no -o ControlPath=none -o LogLevel=quiet -o PasswordAuthentication=no -o ServerAliveInterval=60 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null docker@localhost -o IdentitiesOnly=yes -i /Users/sem.haile/.minikube/machines/minikube/id_rsa -p 51347] /usr/bin/ssh <nil>}
I0306 14:41:14.847847   47093 main.go:141] libmachine: /usr/bin/ssh -F /dev/null -o ConnectionAttempts=3 -o ConnectTimeout=10 -o ControlMaster=no -o ControlPath=none -o LogLevel=quiet -o PasswordAuthentication=no -o ServerAliveInterval=60 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null docker@localhost -o IdentitiesOnly=yes -i /Users/sem.haile/.minikube/machines/minikube/id_rsa -p 51347 -f -NTL 51379:localhost:8443
I0306 14:41:15.201540   47093 kubeadm.go:416] found existing configuration files, will attempt cluster restart
I0306 14:41:15.201579   47093 kubeadm.go:633] restartCluster start
I0306 14:41:15.201717   47093 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0306 14:41:15.324695   47093 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0306 14:41:15.326594   47093 kubeconfig.go:92] found "minikube" server: "https://10.0.2.15:8443"
I0306 14:41:15.329222   47093 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0306 14:41:15.422828   47093 api_server.go:165] Checking apiserver status ...
I0306 14:41:15.422955   47093 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0306 14:41:15.578483   47093 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0306 14:41:16.078705   47093 api_server.go:165] Checking apiserver status ...
I0306 14:41:16.078807   47093 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0306 14:41:16.625012   47093 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0306 14:41:17.078606   47093 api_server.go:165] Checking apiserver status ...
I0306 14:41:17.078751   47093 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0306 14:41:17.277287   47093 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0306 14:41:17.578573   47093 api_server.go:165] Checking apiserver status ...
I0306 14:41:17.578685   47093 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0306 14:41:17.774356   47093 ssh_runner.go:195] Run: sudo egrep ^[0-9]+:freezer: /proc/193841/cgroup
I0306 14:41:17.979186   47093 api_server.go:181] apiserver freezer: "11:freezer:/kubepods/burstable/podea56a38e43622561d394bd631aeb3522/6d337205f63cf34cbd3102c61f337e15b70536bb254f270b1044701bccb91bf0"
I0306 14:41:17.979317   47093 ssh_runner.go:195] Run: sudo cat /sys/fs/cgroup/freezer/kubepods/burstable/podea56a38e43622561d394bd631aeb3522/6d337205f63cf34cbd3102c61f337e15b70536bb254f270b1044701bccb91bf0/freezer.state
I0306 14:41:18.142761   47093 api_server.go:203] freezer state: "THAWED"
I0306 14:41:18.142774   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:41:23.143825   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:41:23.143858   47093 retry.go:31] will retry after 263.082536ms: state is "Stopped"
I0306 14:41:23.407420   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:41:28.408639   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:41:28.408666   47093 retry.go:31] will retry after 381.329545ms: state is "Stopped"
I0306 14:41:28.790606   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:41:33.791898   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:41:33.791921   47093 api_server.go:165] Checking apiserver status ...
I0306 14:41:33.792048   47093 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0306 14:41:33.827366   47093 ssh_runner.go:195] Run: sudo egrep ^[0-9]+:freezer: /proc/193841/cgroup
I0306 14:41:33.854749   47093 api_server.go:181] apiserver freezer: "11:freezer:/kubepods/burstable/podea56a38e43622561d394bd631aeb3522/6d337205f63cf34cbd3102c61f337e15b70536bb254f270b1044701bccb91bf0"
I0306 14:41:33.854854   47093 ssh_runner.go:195] Run: sudo cat /sys/fs/cgroup/freezer/kubepods/burstable/podea56a38e43622561d394bd631aeb3522/6d337205f63cf34cbd3102c61f337e15b70536bb254f270b1044701bccb91bf0/freezer.state
I0306 14:41:33.931279   47093 api_server.go:203] freezer state: "THAWED"
I0306 14:41:33.931297   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:41:38.931805   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:41:38.931830   47093 retry.go:31] will retry after 242.214273ms: state is "Stopped"
I0306 14:41:39.174976   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:41:44.175315   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:41:44.175343   47093 retry.go:31] will retry after 300.724609ms: state is "Stopped"
I0306 14:41:44.476473   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:41:49.477318   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:41:49.477352   47093 kubeadm.go:608] needs reconfigure: apiserver error: timed out waiting for the condition
I0306 14:41:49.477356   47093 kubeadm.go:1120] stopping kube-system containers ...
I0306 14:41:49.477478   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0306 14:41:49.642063   47093 docker.go:456] Stopping containers: [b474fae41d70 bba02e864b5e 1df922a0056b 496ee5683692 0abe95ef90b8 397454e5053f a510af6aa7e2 ad151dd8150f 420c7c4f3ddc 6d337205f63c a07448956823 4b8921c13a3b 61c52f61d76a bf7dd107c4d9 5141b13d4a47 b877d3c7f503 0761582fbdb4 704d802b1f2f 0505f646fd0b e01796093a91 761f91c58a3d fca12dc91446 76ab8b50ec43 3ffc98eeda1b b03f99142d43 615d83ffd96d eb4c0e610fb2 4fe0ad0e4cd3 75f25f248373 ba88a094c356 611d25f69dc4 f35b01efc566 442678e1514e 1249af0030ae af14ee44575f a7fb7f92f805 745ace006802 3c0a328b939f def938aa698e]
I0306 14:41:49.642186   47093 ssh_runner.go:195] Run: docker stop b474fae41d70 bba02e864b5e 1df922a0056b 496ee5683692 0abe95ef90b8 397454e5053f a510af6aa7e2 ad151dd8150f 420c7c4f3ddc 6d337205f63c a07448956823 4b8921c13a3b 61c52f61d76a bf7dd107c4d9 5141b13d4a47 b877d3c7f503 0761582fbdb4 704d802b1f2f 0505f646fd0b e01796093a91 761f91c58a3d fca12dc91446 76ab8b50ec43 3ffc98eeda1b b03f99142d43 615d83ffd96d eb4c0e610fb2 4fe0ad0e4cd3 75f25f248373 ba88a094c356 611d25f69dc4 f35b01efc566 442678e1514e 1249af0030ae af14ee44575f a7fb7f92f805 745ace006802 3c0a328b939f def938aa698e
I0306 14:41:56.227881   47093 ssh_runner.go:235] Completed: docker stop b474fae41d70 bba02e864b5e 1df922a0056b 496ee5683692 0abe95ef90b8 397454e5053f a510af6aa7e2 ad151dd8150f 420c7c4f3ddc 6d337205f63c a07448956823 4b8921c13a3b 61c52f61d76a bf7dd107c4d9 5141b13d4a47 b877d3c7f503 0761582fbdb4 704d802b1f2f 0505f646fd0b e01796093a91 761f91c58a3d fca12dc91446 76ab8b50ec43 3ffc98eeda1b b03f99142d43 615d83ffd96d eb4c0e610fb2 4fe0ad0e4cd3 75f25f248373 ba88a094c356 611d25f69dc4 f35b01efc566 442678e1514e 1249af0030ae af14ee44575f a7fb7f92f805 745ace006802 3c0a328b939f def938aa698e: (6.585639294s)
I0306 14:41:56.228003   47093 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0306 14:41:56.431645   47093 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0306 14:41:56.493567   47093 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Mar  6 10:03 /etc/kubernetes/admin.conf
-rw------- 1 root root 5649 Mar  6 10:03 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Mar  6 10:03 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5597 Mar  6 10:03 /etc/kubernetes/scheduler.conf

I0306 14:41:56.493657   47093 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0306 14:41:56.543967   47093 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0306 14:41:56.662397   47093 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0306 14:41:56.768821   47093 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0306 14:41:56.768916   47093 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0306 14:41:56.815194   47093 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0306 14:41:56.846623   47093 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0306 14:41:56.846716   47093 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0306 14:41:56.873159   47093 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0306 14:41:56.914226   47093 kubeadm.go:710] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0306 14:41:56.914242   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.1:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0306 14:41:57.404656   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.1:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0306 14:42:00.212427   47093 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.1:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (2.807735488s)
I0306 14:42:00.212443   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.1:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0306 14:42:00.672149   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.1:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0306 14:42:00.817081   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.26.1:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0306 14:42:01.005806   47093 api_server.go:51] waiting for apiserver process to appear ...
I0306 14:42:01.005916   47093 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0306 14:42:01.065268   47093 api_server.go:71] duration metric: took 59.464665ms to wait for apiserver process to appear ...
I0306 14:42:01.065277   47093 api_server.go:87] waiting for apiserver healthz status ...
I0306 14:42:01.065287   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:42:06.067728   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:42:06.568426   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:42:11.568832   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:42:12.067904   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:42:17.068226   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:42:17.568155   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:42:22.568461   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:42:23.068681   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:42:28.069078   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": dial tcp 10.0.2.15:8443: i/o timeout (Client.Timeout exceeded while awaiting headers)
I0306 14:42:28.568404   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:42:33.568698   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:42:34.068998   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:42:39.069265   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:42:39.568837   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:42:44.569244   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:42:45.068696   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:42:50.069028   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:42:50.568671   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:42:55.569124   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:42:56.068965   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:43:01.069369   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:43:01.568967   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0306 14:43:01.622954   47093 logs.go:279] 2 containers: [642ce8adbf4b 6d337205f63c]
I0306 14:43:01.623041   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0306 14:43:01.665303   47093 logs.go:279] 2 containers: [2cf8cc8ad825 397454e5053f]
I0306 14:43:01.665386   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0306 14:43:01.702133   47093 logs.go:279] 4 containers: [11a0413bdd3d 5b83e65e078a 1df922a0056b a510af6aa7e2]
I0306 14:43:01.702293   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0306 14:43:01.747450   47093 logs.go:279] 2 containers: [623dc1b8e1aa 0abe95ef90b8]
I0306 14:43:01.747537   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0306 14:43:01.795213   47093 logs.go:279] 2 containers: [d3e945555518 b474fae41d70]
I0306 14:43:01.795304   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0306 14:43:01.834065   47093 logs.go:279] 0 containers: []
W0306 14:43:01.834073   47093 logs.go:281] No container was found matching "kubernetes-dashboard"
I0306 14:43:01.834170   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0306 14:43:01.869621   47093 logs.go:279] 2 containers: [7fb42f6d4909 bba02e864b5e]
I0306 14:43:01.869725   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0306 14:43:01.923271   47093 logs.go:279] 2 containers: [87847107d46c a07448956823]
I0306 14:43:01.923286   47093 logs.go:124] Gathering logs for describe nodes ...
I0306 14:43:01.923293   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.26.1/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0306 14:43:02.322213   47093 logs.go:124] Gathering logs for kube-scheduler [623dc1b8e1aa] ...
I0306 14:43:02.322226   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 623dc1b8e1aa"
I0306 14:43:02.373540   47093 logs.go:124] Gathering logs for kube-controller-manager [a07448956823] ...
I0306 14:43:02.373552   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 a07448956823"
I0306 14:43:02.440482   47093 logs.go:124] Gathering logs for coredns [11a0413bdd3d] ...
I0306 14:43:02.440495   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 11a0413bdd3d"
I0306 14:43:02.501812   47093 logs.go:124] Gathering logs for coredns [5b83e65e078a] ...
I0306 14:43:02.501827   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5b83e65e078a"
I0306 14:43:02.559351   47093 logs.go:124] Gathering logs for kube-proxy [b474fae41d70] ...
I0306 14:43:02.559363   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b474fae41d70"
I0306 14:43:02.609747   47093 logs.go:124] Gathering logs for kube-apiserver [6d337205f63c] ...
I0306 14:43:02.609758   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6d337205f63c"
I0306 14:43:02.692290   47093 logs.go:124] Gathering logs for etcd [2cf8cc8ad825] ...
I0306 14:43:02.692310   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2cf8cc8ad825"
I0306 14:43:02.775149   47093 logs.go:124] Gathering logs for etcd [397454e5053f] ...
I0306 14:43:02.775218   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 397454e5053f"
I0306 14:43:02.907103   47093 logs.go:124] Gathering logs for storage-provisioner [7fb42f6d4909] ...
I0306 14:43:02.907166   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7fb42f6d4909"
I0306 14:43:02.968195   47093 logs.go:124] Gathering logs for storage-provisioner [bba02e864b5e] ...
I0306 14:43:02.968209   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bba02e864b5e"
I0306 14:43:03.014918   47093 logs.go:124] Gathering logs for Docker ...
I0306 14:43:03.014930   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I0306 14:43:03.091439   47093 logs.go:124] Gathering logs for container status ...
I0306 14:43:03.091451   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0306 14:43:03.176587   47093 logs.go:124] Gathering logs for dmesg ...
I0306 14:43:03.176600   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0306 14:43:03.210523   47093 logs.go:124] Gathering logs for kube-apiserver [642ce8adbf4b] ...
I0306 14:43:03.210540   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 642ce8adbf4b"
I0306 14:43:03.346645   47093 logs.go:124] Gathering logs for kube-scheduler [0abe95ef90b8] ...
I0306 14:43:03.346660   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0abe95ef90b8"
I0306 14:43:03.433098   47093 logs.go:124] Gathering logs for kube-proxy [d3e945555518] ...
I0306 14:43:03.433109   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d3e945555518"
I0306 14:43:03.498567   47093 logs.go:124] Gathering logs for kube-controller-manager [87847107d46c] ...
I0306 14:43:03.498579   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 87847107d46c"
I0306 14:43:03.553442   47093 logs.go:124] Gathering logs for kubelet ...
I0306 14:43:03.553475   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0306 14:43:03.607566   47093 logs.go:139] Found kubelet problem: Mar 06 10:04:11 minikube kubelet[73131]: W0306 10:04:11.161049   73131 reflector.go:424] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0306 14:43:03.607696   47093 logs.go:139] Found kubelet problem: Mar 06 10:04:11 minikube kubelet[73131]: E0306 10:04:11.161384   73131 reflector.go:140] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0306 14:43:03.667857   47093 logs.go:124] Gathering logs for coredns [1df922a0056b] ...
I0306 14:43:03.667869   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1df922a0056b"
I0306 14:43:03.712222   47093 logs.go:124] Gathering logs for coredns [a510af6aa7e2] ...
I0306 14:43:03.712233   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 a510af6aa7e2"
I0306 14:43:03.762227   47093 out.go:309] Setting ErrFile to fd 2...
I0306 14:43:03.762245   47093 out.go:348] isatty.IsTerminal(2) = true
W0306 14:43:03.762355   47093 out.go:239] ‚ùå  Problems detected in kubelet:
W0306 14:43:03.762380   47093 out.go:239]     Mar 06 10:04:11 minikube kubelet[73131]: W0306 10:04:11.161049   73131 reflector.go:424] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0306 14:43:03.762390   47093 out.go:239]     Mar 06 10:04:11 minikube kubelet[73131]: E0306 10:04:11.161384   73131 reflector.go:140] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0306 14:43:03.762399   47093 out.go:309] Setting ErrFile to fd 2...
I0306 14:43:03.762405   47093 out.go:348] isatty.IsTerminal(2) = true
I0306 14:43:13.763314   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:43:18.764261   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:43:19.068526   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0306 14:43:19.107244   47093 logs.go:279] 2 containers: [642ce8adbf4b 6d337205f63c]
I0306 14:43:19.107316   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0306 14:43:19.141528   47093 logs.go:279] 2 containers: [2cf8cc8ad825 397454e5053f]
I0306 14:43:19.141603   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0306 14:43:19.173420   47093 logs.go:279] 4 containers: [11a0413bdd3d 5b83e65e078a 1df922a0056b a510af6aa7e2]
I0306 14:43:19.173513   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0306 14:43:19.215008   47093 logs.go:279] 2 containers: [623dc1b8e1aa 0abe95ef90b8]
I0306 14:43:19.215104   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0306 14:43:19.245721   47093 logs.go:279] 2 containers: [d3e945555518 b474fae41d70]
I0306 14:43:19.245820   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0306 14:43:19.289535   47093 logs.go:279] 0 containers: []
W0306 14:43:19.289543   47093 logs.go:281] No container was found matching "kubernetes-dashboard"
I0306 14:43:19.289623   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0306 14:43:19.330544   47093 logs.go:279] 2 containers: [7fb42f6d4909 bba02e864b5e]
I0306 14:43:19.330625   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0306 14:43:19.366536   47093 logs.go:279] 2 containers: [87847107d46c a07448956823]
I0306 14:43:19.366552   47093 logs.go:124] Gathering logs for kube-proxy [b474fae41d70] ...
I0306 14:43:19.366559   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b474fae41d70"
I0306 14:43:19.398401   47093 logs.go:124] Gathering logs for storage-provisioner [7fb42f6d4909] ...
I0306 14:43:19.398411   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7fb42f6d4909"
I0306 14:43:19.430582   47093 logs.go:124] Gathering logs for storage-provisioner [bba02e864b5e] ...
I0306 14:43:19.430591   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bba02e864b5e"
I0306 14:43:19.474143   47093 logs.go:124] Gathering logs for kube-apiserver [6d337205f63c] ...
I0306 14:43:19.474165   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6d337205f63c"
I0306 14:43:19.522756   47093 logs.go:124] Gathering logs for etcd [2cf8cc8ad825] ...
I0306 14:43:19.522766   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2cf8cc8ad825"
I0306 14:43:19.565943   47093 logs.go:124] Gathering logs for coredns [a510af6aa7e2] ...
I0306 14:43:19.565954   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 a510af6aa7e2"
I0306 14:43:19.606620   47093 logs.go:124] Gathering logs for kube-scheduler [0abe95ef90b8] ...
I0306 14:43:19.606629   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0abe95ef90b8"
I0306 14:43:19.645290   47093 logs.go:124] Gathering logs for kube-proxy [d3e945555518] ...
I0306 14:43:19.645300   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d3e945555518"
I0306 14:43:19.692872   47093 logs.go:124] Gathering logs for Docker ...
I0306 14:43:19.692887   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I0306 14:43:19.750989   47093 logs.go:124] Gathering logs for container status ...
I0306 14:43:19.751006   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0306 14:43:19.836133   47093 logs.go:124] Gathering logs for coredns [11a0413bdd3d] ...
I0306 14:43:19.836145   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 11a0413bdd3d"
I0306 14:43:19.907446   47093 logs.go:124] Gathering logs for coredns [5b83e65e078a] ...
I0306 14:43:19.907456   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5b83e65e078a"
I0306 14:43:19.969961   47093 logs.go:124] Gathering logs for coredns [1df922a0056b] ...
I0306 14:43:19.969972   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1df922a0056b"
I0306 14:43:20.003316   47093 logs.go:124] Gathering logs for kube-scheduler [623dc1b8e1aa] ...
I0306 14:43:20.003327   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 623dc1b8e1aa"
I0306 14:43:20.052834   47093 logs.go:124] Gathering logs for dmesg ...
I0306 14:43:20.052846   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0306 14:43:20.084722   47093 logs.go:124] Gathering logs for kube-apiserver [642ce8adbf4b] ...
I0306 14:43:20.084732   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 642ce8adbf4b"
I0306 14:43:20.187233   47093 logs.go:124] Gathering logs for etcd [397454e5053f] ...
I0306 14:43:20.187244   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 397454e5053f"
I0306 14:43:20.246144   47093 logs.go:124] Gathering logs for kube-controller-manager [87847107d46c] ...
I0306 14:43:20.246174   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 87847107d46c"
I0306 14:43:20.303272   47093 logs.go:124] Gathering logs for kube-controller-manager [a07448956823] ...
I0306 14:43:20.303284   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 a07448956823"
I0306 14:43:20.350535   47093 logs.go:124] Gathering logs for kubelet ...
I0306 14:43:20.350553   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0306 14:43:20.388218   47093 logs.go:139] Found kubelet problem: Mar 06 10:04:11 minikube kubelet[73131]: W0306 10:04:11.161049   73131 reflector.go:424] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0306 14:43:20.388420   47093 logs.go:139] Found kubelet problem: Mar 06 10:04:11 minikube kubelet[73131]: E0306 10:04:11.161384   73131 reflector.go:140] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0306 14:43:20.443805   47093 logs.go:124] Gathering logs for describe nodes ...
I0306 14:43:20.443819   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.26.1/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0306 14:43:20.630544   47093 out.go:309] Setting ErrFile to fd 2...
I0306 14:43:20.630562   47093 out.go:348] isatty.IsTerminal(2) = true
W0306 14:43:20.630642   47093 out.go:239] ‚ùå  Problems detected in kubelet:
W0306 14:43:20.630655   47093 out.go:239]     Mar 06 10:04:11 minikube kubelet[73131]: W0306 10:04:11.161049   73131 reflector.go:424] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0306 14:43:20.630661   47093 out.go:239]     Mar 06 10:04:11 minikube kubelet[73131]: E0306 10:04:11.161384   73131 reflector.go:140] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0306 14:43:20.630665   47093 out.go:309] Setting ErrFile to fd 2...
I0306 14:43:20.630671   47093 out.go:348] isatty.IsTerminal(2) = true
I0306 14:43:30.631781   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:43:35.632657   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:43:36.069580   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0306 14:43:36.123298   47093 logs.go:279] 2 containers: [642ce8adbf4b 6d337205f63c]
I0306 14:43:36.123374   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0306 14:43:36.163354   47093 logs.go:279] 2 containers: [2cf8cc8ad825 397454e5053f]
I0306 14:43:36.181196   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0306 14:43:36.216381   47093 logs.go:279] 4 containers: [11a0413bdd3d 5b83e65e078a 1df922a0056b a510af6aa7e2]
I0306 14:43:36.216458   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0306 14:43:36.245703   47093 logs.go:279] 2 containers: [623dc1b8e1aa 0abe95ef90b8]
I0306 14:43:36.245780   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0306 14:43:36.282951   47093 logs.go:279] 2 containers: [d3e945555518 b474fae41d70]
I0306 14:43:36.283028   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0306 14:43:36.316574   47093 logs.go:279] 0 containers: []
W0306 14:43:36.316584   47093 logs.go:281] No container was found matching "kubernetes-dashboard"
I0306 14:43:36.316665   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0306 14:43:36.344957   47093 logs.go:279] 2 containers: [7fb42f6d4909 bba02e864b5e]
I0306 14:43:36.345032   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0306 14:43:36.374144   47093 logs.go:279] 2 containers: [87847107d46c a07448956823]
I0306 14:43:36.374162   47093 logs.go:124] Gathering logs for Docker ...
I0306 14:43:36.374168   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I0306 14:43:36.436485   47093 logs.go:124] Gathering logs for dmesg ...
I0306 14:43:36.436498   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0306 14:43:36.468824   47093 logs.go:124] Gathering logs for kube-scheduler [623dc1b8e1aa] ...
I0306 14:43:36.468836   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 623dc1b8e1aa"
I0306 14:43:36.531278   47093 logs.go:124] Gathering logs for storage-provisioner [7fb42f6d4909] ...
I0306 14:43:36.531298   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7fb42f6d4909"
I0306 14:43:36.590581   47093 logs.go:124] Gathering logs for storage-provisioner [bba02e864b5e] ...
I0306 14:43:36.590594   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bba02e864b5e"
I0306 14:43:36.641584   47093 logs.go:124] Gathering logs for kube-controller-manager [87847107d46c] ...
I0306 14:43:36.641594   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 87847107d46c"
I0306 14:43:36.696552   47093 logs.go:124] Gathering logs for kube-controller-manager [a07448956823] ...
I0306 14:43:36.696564   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 a07448956823"
I0306 14:43:36.744121   47093 logs.go:124] Gathering logs for describe nodes ...
I0306 14:43:36.744133   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.26.1/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0306 14:43:36.928155   47093 logs.go:124] Gathering logs for coredns [5b83e65e078a] ...
I0306 14:43:36.928165   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5b83e65e078a"
I0306 14:43:36.960762   47093 logs.go:124] Gathering logs for coredns [1df922a0056b] ...
I0306 14:43:36.960772   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1df922a0056b"
I0306 14:43:36.996435   47093 logs.go:124] Gathering logs for kube-scheduler [0abe95ef90b8] ...
I0306 14:43:36.996445   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0abe95ef90b8"
I0306 14:43:37.033705   47093 logs.go:124] Gathering logs for kube-proxy [b474fae41d70] ...
I0306 14:43:37.033716   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b474fae41d70"
I0306 14:43:37.065411   47093 logs.go:124] Gathering logs for kubelet ...
I0306 14:43:37.065421   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0306 14:43:37.104027   47093 logs.go:139] Found kubelet problem: Mar 06 10:04:11 minikube kubelet[73131]: W0306 10:04:11.161049   73131 reflector.go:424] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0306 14:43:37.104144   47093 logs.go:139] Found kubelet problem: Mar 06 10:04:11 minikube kubelet[73131]: E0306 10:04:11.161384   73131 reflector.go:140] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0306 14:43:37.151430   47093 logs.go:124] Gathering logs for etcd [2cf8cc8ad825] ...
I0306 14:43:37.151500   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2cf8cc8ad825"
I0306 14:43:37.192760   47093 logs.go:124] Gathering logs for coredns [a510af6aa7e2] ...
I0306 14:43:37.192771   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 a510af6aa7e2"
I0306 14:43:37.234039   47093 logs.go:124] Gathering logs for kube-apiserver [642ce8adbf4b] ...
I0306 14:43:37.234050   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 642ce8adbf4b"
I0306 14:43:37.295007   47093 logs.go:124] Gathering logs for kube-apiserver [6d337205f63c] ...
I0306 14:43:37.295019   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6d337205f63c"
I0306 14:43:37.365948   47093 logs.go:124] Gathering logs for etcd [397454e5053f] ...
I0306 14:43:37.365962   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 397454e5053f"
I0306 14:43:37.410705   47093 logs.go:124] Gathering logs for coredns [11a0413bdd3d] ...
I0306 14:43:37.410716   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 11a0413bdd3d"
I0306 14:43:37.443768   47093 logs.go:124] Gathering logs for kube-proxy [d3e945555518] ...
I0306 14:43:37.443778   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d3e945555518"
I0306 14:43:37.481949   47093 logs.go:124] Gathering logs for container status ...
I0306 14:43:37.481960   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0306 14:43:37.553163   47093 out.go:309] Setting ErrFile to fd 2...
I0306 14:43:37.553182   47093 out.go:348] isatty.IsTerminal(2) = true
W0306 14:43:37.553265   47093 out.go:239] ‚ùå  Problems detected in kubelet:
W0306 14:43:37.553283   47093 out.go:239]     Mar 06 10:04:11 minikube kubelet[73131]: W0306 10:04:11.161049   73131 reflector.go:424] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0306 14:43:37.553288   47093 out.go:239]     Mar 06 10:04:11 minikube kubelet[73131]: E0306 10:04:11.161384   73131 reflector.go:140] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0306 14:43:37.553292   47093 out.go:309] Setting ErrFile to fd 2...
I0306 14:43:37.553299   47093 out.go:348] isatty.IsTerminal(2) = true
I0306 14:43:47.555145   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:43:52.556509   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:43:52.569009   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0306 14:43:52.650637   47093 logs.go:279] 2 containers: [642ce8adbf4b 6d337205f63c]
I0306 14:43:52.650714   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0306 14:43:52.745850   47093 logs.go:279] 2 containers: [2cf8cc8ad825 397454e5053f]
I0306 14:43:52.746285   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0306 14:43:52.876320   47093 logs.go:279] 4 containers: [11a0413bdd3d 5b83e65e078a 1df922a0056b a510af6aa7e2]
I0306 14:43:52.876418   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0306 14:43:53.038149   47093 logs.go:279] 2 containers: [623dc1b8e1aa 0abe95ef90b8]
I0306 14:43:53.038248   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0306 14:43:53.095969   47093 logs.go:279] 2 containers: [d3e945555518 b474fae41d70]
I0306 14:43:53.096102   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0306 14:43:53.143881   47093 logs.go:279] 0 containers: []
W0306 14:43:53.143890   47093 logs.go:281] No container was found matching "kubernetes-dashboard"
I0306 14:43:53.143978   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0306 14:43:53.199259   47093 logs.go:279] 2 containers: [7fb42f6d4909 bba02e864b5e]
I0306 14:43:53.199336   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0306 14:43:53.298880   47093 logs.go:279] 2 containers: [87847107d46c a07448956823]
I0306 14:43:53.298895   47093 logs.go:124] Gathering logs for kube-controller-manager [a07448956823] ...
I0306 14:43:53.298901   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 a07448956823"
I0306 14:43:53.577403   47093 logs.go:124] Gathering logs for Docker ...
I0306 14:43:53.577418   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I0306 14:43:53.668459   47093 logs.go:124] Gathering logs for etcd [2cf8cc8ad825] ...
I0306 14:43:53.668473   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2cf8cc8ad825"
I0306 14:43:53.723519   47093 logs.go:124] Gathering logs for storage-provisioner [7fb42f6d4909] ...
I0306 14:43:53.723535   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7fb42f6d4909"
I0306 14:43:53.789016   47093 logs.go:124] Gathering logs for kube-proxy [b474fae41d70] ...
I0306 14:43:53.789029   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b474fae41d70"
I0306 14:43:53.830499   47093 logs.go:124] Gathering logs for storage-provisioner [bba02e864b5e] ...
I0306 14:43:53.830513   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bba02e864b5e"
I0306 14:43:53.896111   47093 logs.go:124] Gathering logs for container status ...
I0306 14:43:53.896125   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0306 14:43:54.165400   47093 logs.go:124] Gathering logs for coredns [5b83e65e078a] ...
I0306 14:43:54.165412   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5b83e65e078a"
I0306 14:43:54.208657   47093 logs.go:124] Gathering logs for coredns [1df922a0056b] ...
I0306 14:43:54.208669   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1df922a0056b"
I0306 14:43:54.251142   47093 logs.go:124] Gathering logs for etcd [397454e5053f] ...
I0306 14:43:54.251154   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 397454e5053f"
I0306 14:43:54.321808   47093 logs.go:124] Gathering logs for kube-scheduler [623dc1b8e1aa] ...
I0306 14:43:54.321833   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 623dc1b8e1aa"
I0306 14:43:54.370409   47093 logs.go:124] Gathering logs for kube-scheduler [0abe95ef90b8] ...
I0306 14:43:54.370422   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0abe95ef90b8"
I0306 14:43:54.421781   47093 logs.go:124] Gathering logs for kube-proxy [d3e945555518] ...
I0306 14:43:54.421795   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d3e945555518"
I0306 14:43:54.492231   47093 logs.go:124] Gathering logs for dmesg ...
I0306 14:43:54.492243   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0306 14:43:54.571677   47093 logs.go:124] Gathering logs for describe nodes ...
I0306 14:43:54.571688   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.26.1/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0306 14:43:54.806923   47093 logs.go:124] Gathering logs for kube-apiserver [6d337205f63c] ...
I0306 14:43:54.806935   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6d337205f63c"
I0306 14:43:54.902198   47093 logs.go:124] Gathering logs for coredns [11a0413bdd3d] ...
I0306 14:43:54.902215   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 11a0413bdd3d"
I0306 14:43:54.980216   47093 logs.go:124] Gathering logs for coredns [a510af6aa7e2] ...
I0306 14:43:54.980231   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 a510af6aa7e2"
I0306 14:43:55.060929   47093 logs.go:124] Gathering logs for kube-controller-manager [87847107d46c] ...
I0306 14:43:55.060942   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 87847107d46c"
I0306 14:43:55.139727   47093 logs.go:124] Gathering logs for kubelet ...
I0306 14:43:55.139741   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0306 14:43:55.184584   47093 logs.go:139] Found kubelet problem: Mar 06 10:04:11 minikube kubelet[73131]: W0306 10:04:11.161049   73131 reflector.go:424] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0306 14:43:55.184716   47093 logs.go:139] Found kubelet problem: Mar 06 10:04:11 minikube kubelet[73131]: E0306 10:04:11.161384   73131 reflector.go:140] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0306 14:43:55.238885   47093 logs.go:124] Gathering logs for kube-apiserver [642ce8adbf4b] ...
I0306 14:43:55.238898   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 642ce8adbf4b"
I0306 14:43:55.303384   47093 out.go:309] Setting ErrFile to fd 2...
I0306 14:43:55.303403   47093 out.go:348] isatty.IsTerminal(2) = true
W0306 14:43:55.303491   47093 out.go:239] ‚ùå  Problems detected in kubelet:
W0306 14:43:55.303507   47093 out.go:239]     Mar 06 10:04:11 minikube kubelet[73131]: W0306 10:04:11.161049   73131 reflector.go:424] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0306 14:43:55.303513   47093 out.go:239]     Mar 06 10:04:11 minikube kubelet[73131]: E0306 10:04:11.161384   73131 reflector.go:140] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0306 14:43:55.303517   47093 out.go:309] Setting ErrFile to fd 2...
I0306 14:43:55.303525   47093 out.go:348] isatty.IsTerminal(2) = true
I0306 14:44:05.304741   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:44:10.305814   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:44:10.569381   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0306 14:44:10.619563   47093 logs.go:279] 2 containers: [642ce8adbf4b 6d337205f63c]
I0306 14:44:10.619645   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0306 14:44:10.652982   47093 logs.go:279] 2 containers: [2cf8cc8ad825 397454e5053f]
I0306 14:44:10.653054   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0306 14:44:10.692635   47093 logs.go:279] 4 containers: [11a0413bdd3d 5b83e65e078a 1df922a0056b a510af6aa7e2]
I0306 14:44:10.692709   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0306 14:44:10.724672   47093 logs.go:279] 2 containers: [623dc1b8e1aa 0abe95ef90b8]
I0306 14:44:10.724744   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0306 14:44:10.755558   47093 logs.go:279] 2 containers: [d3e945555518 b474fae41d70]
I0306 14:44:10.755628   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0306 14:44:10.785883   47093 logs.go:279] 0 containers: []
W0306 14:44:10.785890   47093 logs.go:281] No container was found matching "kubernetes-dashboard"
I0306 14:44:10.785983   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0306 14:44:10.819372   47093 logs.go:279] 2 containers: [7fb42f6d4909 bba02e864b5e]
I0306 14:44:10.819455   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0306 14:44:10.856758   47093 logs.go:279] 2 containers: [87847107d46c a07448956823]
I0306 14:44:10.856774   47093 logs.go:124] Gathering logs for etcd [397454e5053f] ...
I0306 14:44:10.856781   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 397454e5053f"
I0306 14:44:10.903100   47093 logs.go:124] Gathering logs for Docker ...
I0306 14:44:10.903111   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I0306 14:44:10.975525   47093 logs.go:124] Gathering logs for kube-scheduler [623dc1b8e1aa] ...
I0306 14:44:10.975538   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 623dc1b8e1aa"
I0306 14:44:11.017997   47093 logs.go:124] Gathering logs for coredns [a510af6aa7e2] ...
I0306 14:44:11.018007   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 a510af6aa7e2"
I0306 14:44:11.070458   47093 logs.go:124] Gathering logs for kube-scheduler [0abe95ef90b8] ...
I0306 14:44:11.070468   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0abe95ef90b8"
I0306 14:44:11.116177   47093 logs.go:124] Gathering logs for kube-proxy [d3e945555518] ...
I0306 14:44:11.116188   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d3e945555518"
I0306 14:44:11.149567   47093 logs.go:124] Gathering logs for kube-proxy [b474fae41d70] ...
I0306 14:44:11.149577   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b474fae41d70"
I0306 14:44:11.194610   47093 logs.go:124] Gathering logs for storage-provisioner [7fb42f6d4909] ...
I0306 14:44:11.194620   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7fb42f6d4909"
I0306 14:44:11.236325   47093 logs.go:124] Gathering logs for kubelet ...
I0306 14:44:11.236334   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0306 14:44:11.300773   47093 logs.go:139] Found kubelet problem: Mar 06 10:04:11 minikube kubelet[73131]: W0306 10:04:11.161049   73131 reflector.go:424] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0306 14:44:11.300891   47093 logs.go:139] Found kubelet problem: Mar 06 10:04:11 minikube kubelet[73131]: E0306 10:04:11.161384   73131 reflector.go:140] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0306 14:44:11.348151   47093 logs.go:124] Gathering logs for describe nodes ...
I0306 14:44:11.348163   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.26.1/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0306 14:44:11.566608   47093 logs.go:124] Gathering logs for coredns [1df922a0056b] ...
I0306 14:44:11.566620   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1df922a0056b"
I0306 14:44:11.640478   47093 logs.go:124] Gathering logs for container status ...
I0306 14:44:11.640488   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0306 14:44:11.692120   47093 logs.go:124] Gathering logs for etcd [2cf8cc8ad825] ...
I0306 14:44:11.692130   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2cf8cc8ad825"
I0306 14:44:11.733145   47093 logs.go:124] Gathering logs for coredns [11a0413bdd3d] ...
I0306 14:44:11.733155   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 11a0413bdd3d"
I0306 14:44:11.775924   47093 logs.go:124] Gathering logs for coredns [5b83e65e078a] ...
I0306 14:44:11.775933   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5b83e65e078a"
I0306 14:44:11.814290   47093 logs.go:124] Gathering logs for storage-provisioner [bba02e864b5e] ...
I0306 14:44:11.814300   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bba02e864b5e"
I0306 14:44:11.865853   47093 logs.go:124] Gathering logs for kube-controller-manager [87847107d46c] ...
I0306 14:44:11.865865   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 87847107d46c"
I0306 14:44:11.907878   47093 logs.go:124] Gathering logs for dmesg ...
I0306 14:44:11.907888   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0306 14:44:11.936272   47093 logs.go:124] Gathering logs for kube-apiserver [642ce8adbf4b] ...
I0306 14:44:11.936286   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 642ce8adbf4b"
I0306 14:44:11.980887   47093 logs.go:124] Gathering logs for kube-apiserver [6d337205f63c] ...
I0306 14:44:11.980898   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6d337205f63c"
I0306 14:44:12.031488   47093 logs.go:124] Gathering logs for kube-controller-manager [a07448956823] ...
I0306 14:44:12.031518   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 a07448956823"
I0306 14:44:12.078773   47093 out.go:309] Setting ErrFile to fd 2...
I0306 14:44:12.078792   47093 out.go:348] isatty.IsTerminal(2) = true
W0306 14:44:12.078878   47093 out.go:239] ‚ùå  Problems detected in kubelet:
W0306 14:44:12.078894   47093 out.go:239]     Mar 06 10:04:11 minikube kubelet[73131]: W0306 10:04:11.161049   73131 reflector.go:424] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0306 14:44:12.078899   47093 out.go:239]     Mar 06 10:04:11 minikube kubelet[73131]: E0306 10:04:11.161384   73131 reflector.go:140] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0306 14:44:12.078903   47093 out.go:309] Setting ErrFile to fd 2...
I0306 14:44:12.078909   47093 out.go:348] isatty.IsTerminal(2) = true
I0306 14:44:22.079445   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:44:27.080151   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:44:27.569438   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0306 14:44:27.609801   47093 logs.go:279] 2 containers: [642ce8adbf4b 6d337205f63c]
I0306 14:44:27.609864   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0306 14:44:27.657588   47093 logs.go:279] 2 containers: [2cf8cc8ad825 397454e5053f]
I0306 14:44:27.657682   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0306 14:44:27.693151   47093 logs.go:279] 4 containers: [11a0413bdd3d 5b83e65e078a 1df922a0056b a510af6aa7e2]
I0306 14:44:27.693233   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0306 14:44:27.723394   47093 logs.go:279] 2 containers: [623dc1b8e1aa 0abe95ef90b8]
I0306 14:44:27.723470   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0306 14:44:27.755851   47093 logs.go:279] 2 containers: [d3e945555518 b474fae41d70]
I0306 14:44:27.755930   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0306 14:44:27.788403   47093 logs.go:279] 0 containers: []
W0306 14:44:27.788410   47093 logs.go:281] No container was found matching "kubernetes-dashboard"
I0306 14:44:27.788474   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0306 14:44:27.817684   47093 logs.go:279] 2 containers: [7fb42f6d4909 bba02e864b5e]
I0306 14:44:27.817753   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0306 14:44:27.844671   47093 logs.go:279] 2 containers: [87847107d46c a07448956823]
I0306 14:44:27.844684   47093 logs.go:124] Gathering logs for kube-controller-manager [a07448956823] ...
I0306 14:44:27.844690   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 a07448956823"
I0306 14:44:27.895984   47093 logs.go:124] Gathering logs for container status ...
I0306 14:44:27.895996   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0306 14:44:27.948385   47093 logs.go:124] Gathering logs for describe nodes ...
I0306 14:44:27.948395   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.26.1/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0306 14:44:28.293193   47093 logs.go:124] Gathering logs for kube-apiserver [642ce8adbf4b] ...
I0306 14:44:28.293205   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 642ce8adbf4b"
I0306 14:44:28.362734   47093 logs.go:124] Gathering logs for kube-proxy [d3e945555518] ...
I0306 14:44:28.362746   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d3e945555518"
I0306 14:44:28.400688   47093 logs.go:124] Gathering logs for coredns [1df922a0056b] ...
I0306 14:44:28.400699   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1df922a0056b"
I0306 14:44:28.449697   47093 logs.go:124] Gathering logs for coredns [a510af6aa7e2] ...
I0306 14:44:28.449717   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 a510af6aa7e2"
I0306 14:44:29.835515   47093 ssh_runner.go:235] Completed: /bin/bash -c "docker logs --tail 400 a510af6aa7e2": (1.385770365s)
I0306 14:44:29.836286   47093 logs.go:124] Gathering logs for storage-provisioner [7fb42f6d4909] ...
I0306 14:44:29.836299   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7fb42f6d4909"
I0306 14:44:30.031609   47093 logs.go:124] Gathering logs for storage-provisioner [bba02e864b5e] ...
I0306 14:44:30.031622   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bba02e864b5e"
I0306 14:44:31.754762   47093 ssh_runner.go:235] Completed: /bin/bash -c "docker logs --tail 400 bba02e864b5e": (1.723114503s)
I0306 14:44:31.755230   47093 logs.go:124] Gathering logs for Docker ...
I0306 14:44:31.755237   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I0306 14:44:31.874307   47093 logs.go:124] Gathering logs for kubelet ...
I0306 14:44:31.874321   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0306 14:44:31.967249   47093 logs.go:139] Found kubelet problem: Mar 06 10:04:11 minikube kubelet[73131]: W0306 10:04:11.161049   73131 reflector.go:424] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0306 14:44:31.967389   47093 logs.go:139] Found kubelet problem: Mar 06 10:04:11 minikube kubelet[73131]: E0306 10:04:11.161384   73131 reflector.go:140] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0306 14:44:32.026705   47093 logs.go:124] Gathering logs for kube-apiserver [6d337205f63c] ...
I0306 14:44:32.026719   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6d337205f63c"
I0306 14:44:32.167315   47093 logs.go:124] Gathering logs for coredns [11a0413bdd3d] ...
I0306 14:44:32.167336   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 11a0413bdd3d"
I0306 14:44:32.209152   47093 logs.go:124] Gathering logs for kube-proxy [b474fae41d70] ...
I0306 14:44:32.209163   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b474fae41d70"
I0306 14:44:32.253757   47093 logs.go:124] Gathering logs for dmesg ...
I0306 14:44:32.253769   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0306 14:44:32.304993   47093 logs.go:124] Gathering logs for coredns [5b83e65e078a] ...
I0306 14:44:32.305007   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5b83e65e078a"
I0306 14:44:32.342993   47093 logs.go:124] Gathering logs for kube-scheduler [623dc1b8e1aa] ...
I0306 14:44:32.343004   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 623dc1b8e1aa"
I0306 14:44:32.391779   47093 logs.go:124] Gathering logs for kube-controller-manager [87847107d46c] ...
I0306 14:44:32.391792   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 87847107d46c"
I0306 14:44:32.449852   47093 logs.go:124] Gathering logs for etcd [2cf8cc8ad825] ...
I0306 14:44:32.449865   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2cf8cc8ad825"
I0306 14:44:32.504844   47093 logs.go:124] Gathering logs for etcd [397454e5053f] ...
I0306 14:44:32.504859   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 397454e5053f"
I0306 14:44:32.554638   47093 logs.go:124] Gathering logs for kube-scheduler [0abe95ef90b8] ...
I0306 14:44:32.554650   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0abe95ef90b8"
I0306 14:44:32.602315   47093 out.go:309] Setting ErrFile to fd 2...
I0306 14:44:32.602334   47093 out.go:348] isatty.IsTerminal(2) = true
W0306 14:44:32.602422   47093 out.go:239] ‚ùå  Problems detected in kubelet:
W0306 14:44:32.602437   47093 out.go:239]     Mar 06 10:04:11 minikube kubelet[73131]: W0306 10:04:11.161049   73131 reflector.go:424] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0306 14:44:32.602443   47093 out.go:239]     Mar 06 10:04:11 minikube kubelet[73131]: E0306 10:04:11.161384   73131 reflector.go:140] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0306 14:44:32.602447   47093 out.go:309] Setting ErrFile to fd 2...
I0306 14:44:32.602455   47093 out.go:348] isatty.IsTerminal(2) = true
I0306 14:44:42.604081   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:44:47.605624   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:44:48.069664   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0306 14:44:48.111689   47093 logs.go:279] 2 containers: [642ce8adbf4b 6d337205f63c]
I0306 14:44:48.111783   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0306 14:44:48.147344   47093 logs.go:279] 2 containers: [2cf8cc8ad825 397454e5053f]
I0306 14:44:48.147430   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0306 14:44:48.185871   47093 logs.go:279] 4 containers: [11a0413bdd3d 5b83e65e078a 1df922a0056b a510af6aa7e2]
I0306 14:44:48.186257   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0306 14:44:48.225326   47093 logs.go:279] 2 containers: [623dc1b8e1aa 0abe95ef90b8]
I0306 14:44:48.225389   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0306 14:44:48.262264   47093 logs.go:279] 2 containers: [d3e945555518 b474fae41d70]
I0306 14:44:48.262337   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0306 14:44:48.305308   47093 logs.go:279] 0 containers: []
W0306 14:44:48.305317   47093 logs.go:281] No container was found matching "kubernetes-dashboard"
I0306 14:44:48.305493   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0306 14:44:48.361245   47093 logs.go:279] 2 containers: [7fb42f6d4909 bba02e864b5e]
I0306 14:44:48.361332   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0306 14:44:48.400544   47093 logs.go:279] 2 containers: [87847107d46c a07448956823]
I0306 14:44:48.400559   47093 logs.go:124] Gathering logs for dmesg ...
I0306 14:44:48.400565   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0306 14:44:48.432912   47093 logs.go:124] Gathering logs for describe nodes ...
I0306 14:44:48.432923   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.26.1/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0306 14:44:48.671855   47093 logs.go:124] Gathering logs for kube-proxy [b474fae41d70] ...
I0306 14:44:48.671867   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b474fae41d70"
I0306 14:44:48.722371   47093 logs.go:124] Gathering logs for Docker ...
I0306 14:44:48.722383   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I0306 14:44:48.791940   47093 logs.go:124] Gathering logs for kube-apiserver [6d337205f63c] ...
I0306 14:44:48.791953   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6d337205f63c"
I0306 14:44:48.858890   47093 logs.go:124] Gathering logs for etcd [397454e5053f] ...
I0306 14:44:48.858902   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 397454e5053f"
I0306 14:44:48.904833   47093 logs.go:124] Gathering logs for kube-proxy [d3e945555518] ...
I0306 14:44:48.904844   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d3e945555518"
I0306 14:44:48.950828   47093 logs.go:124] Gathering logs for container status ...
I0306 14:44:48.950839   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0306 14:44:49.034203   47093 logs.go:124] Gathering logs for kube-scheduler [0abe95ef90b8] ...
I0306 14:44:49.034233   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0abe95ef90b8"
I0306 14:44:49.101843   47093 logs.go:124] Gathering logs for kube-controller-manager [87847107d46c] ...
I0306 14:44:49.101855   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 87847107d46c"
I0306 14:44:49.177377   47093 logs.go:124] Gathering logs for kube-controller-manager [a07448956823] ...
I0306 14:44:49.177391   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 a07448956823"
I0306 14:44:49.233686   47093 logs.go:124] Gathering logs for kube-apiserver [642ce8adbf4b] ...
I0306 14:44:49.233698   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 642ce8adbf4b"
I0306 14:44:49.310962   47093 logs.go:124] Gathering logs for etcd [2cf8cc8ad825] ...
I0306 14:44:49.310975   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2cf8cc8ad825"
I0306 14:44:49.392207   47093 logs.go:124] Gathering logs for coredns [5b83e65e078a] ...
I0306 14:44:49.392220   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5b83e65e078a"
I0306 14:44:49.447243   47093 logs.go:124] Gathering logs for coredns [a510af6aa7e2] ...
I0306 14:44:49.447256   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 a510af6aa7e2"
I0306 14:44:49.486045   47093 logs.go:124] Gathering logs for kube-scheduler [623dc1b8e1aa] ...
I0306 14:44:49.486056   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 623dc1b8e1aa"
I0306 14:44:49.527945   47093 logs.go:124] Gathering logs for kubelet ...
I0306 14:44:49.527956   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0306 14:44:49.569777   47093 logs.go:139] Found kubelet problem: Mar 06 10:04:11 minikube kubelet[73131]: W0306 10:04:11.161049   73131 reflector.go:424] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0306 14:44:49.569896   47093 logs.go:139] Found kubelet problem: Mar 06 10:04:11 minikube kubelet[73131]: E0306 10:04:11.161384   73131 reflector.go:140] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0306 14:44:49.620118   47093 logs.go:124] Gathering logs for coredns [11a0413bdd3d] ...
I0306 14:44:49.620130   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 11a0413bdd3d"
I0306 14:44:49.677480   47093 logs.go:124] Gathering logs for coredns [1df922a0056b] ...
I0306 14:44:49.677491   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1df922a0056b"
I0306 14:44:49.733704   47093 logs.go:124] Gathering logs for storage-provisioner [7fb42f6d4909] ...
I0306 14:44:49.733717   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7fb42f6d4909"
I0306 14:44:49.777846   47093 logs.go:124] Gathering logs for storage-provisioner [bba02e864b5e] ...
I0306 14:44:49.777858   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bba02e864b5e"
I0306 14:44:49.820002   47093 out.go:309] Setting ErrFile to fd 2...
I0306 14:44:49.820020   47093 out.go:348] isatty.IsTerminal(2) = true
W0306 14:44:49.820095   47093 out.go:239] ‚ùå  Problems detected in kubelet:
W0306 14:44:49.820109   47093 out.go:239]     Mar 06 10:04:11 minikube kubelet[73131]: W0306 10:04:11.161049   73131 reflector.go:424] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0306 14:44:49.820116   47093 out.go:239]     Mar 06 10:04:11 minikube kubelet[73131]: E0306 10:04:11.161384   73131 reflector.go:140] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0306 14:44:49.820121   47093 out.go:309] Setting ErrFile to fd 2...
I0306 14:44:49.820127   47093 out.go:348] isatty.IsTerminal(2) = true
I0306 14:44:59.821939   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:45:04.823156   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:45:05.069719   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0306 14:45:05.101737   47093 logs.go:279] 2 containers: [642ce8adbf4b 6d337205f63c]
I0306 14:45:05.101808   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0306 14:45:05.134403   47093 logs.go:279] 2 containers: [2cf8cc8ad825 397454e5053f]
I0306 14:45:05.134471   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0306 14:45:05.165429   47093 logs.go:279] 4 containers: [11a0413bdd3d 5b83e65e078a 1df922a0056b a510af6aa7e2]
I0306 14:45:05.165541   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0306 14:45:05.198675   47093 logs.go:279] 2 containers: [623dc1b8e1aa 0abe95ef90b8]
I0306 14:45:05.198753   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0306 14:45:05.226542   47093 logs.go:279] 2 containers: [d3e945555518 b474fae41d70]
I0306 14:45:05.226626   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0306 14:45:05.279304   47093 logs.go:279] 0 containers: []
W0306 14:45:05.279312   47093 logs.go:281] No container was found matching "kubernetes-dashboard"
I0306 14:45:05.279382   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0306 14:45:05.322972   47093 logs.go:279] 2 containers: [7fb42f6d4909 bba02e864b5e]
I0306 14:45:05.323058   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0306 14:45:05.407626   47093 logs.go:279] 2 containers: [87847107d46c a07448956823]
I0306 14:45:05.407641   47093 logs.go:124] Gathering logs for kube-apiserver [642ce8adbf4b] ...
I0306 14:45:05.407648   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 642ce8adbf4b"
I0306 14:45:05.468633   47093 logs.go:124] Gathering logs for kube-apiserver [6d337205f63c] ...
I0306 14:45:05.468644   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6d337205f63c"
I0306 14:45:05.526265   47093 logs.go:124] Gathering logs for coredns [5b83e65e078a] ...
I0306 14:45:05.526277   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5b83e65e078a"
I0306 14:45:05.569714   47093 logs.go:124] Gathering logs for kube-scheduler [0abe95ef90b8] ...
I0306 14:45:05.569724   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0abe95ef90b8"
I0306 14:45:05.615280   47093 logs.go:124] Gathering logs for dmesg ...
I0306 14:45:05.615292   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0306 14:45:05.641679   47093 logs.go:124] Gathering logs for coredns [11a0413bdd3d] ...
I0306 14:45:05.641689   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 11a0413bdd3d"
I0306 14:45:05.675698   47093 logs.go:124] Gathering logs for kube-scheduler [623dc1b8e1aa] ...
I0306 14:45:05.675708   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 623dc1b8e1aa"
I0306 14:45:05.711347   47093 logs.go:124] Gathering logs for storage-provisioner [bba02e864b5e] ...
I0306 14:45:05.711357   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bba02e864b5e"
I0306 14:45:05.741551   47093 logs.go:124] Gathering logs for etcd [397454e5053f] ...
I0306 14:45:05.741562   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 397454e5053f"
I0306 14:45:05.787779   47093 logs.go:124] Gathering logs for coredns [a510af6aa7e2] ...
I0306 14:45:05.787790   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 a510af6aa7e2"
I0306 14:45:05.841261   47093 logs.go:124] Gathering logs for kube-proxy [d3e945555518] ...
I0306 14:45:05.841275   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d3e945555518"
I0306 14:45:05.897491   47093 logs.go:124] Gathering logs for kube-proxy [b474fae41d70] ...
I0306 14:45:05.897504   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b474fae41d70"
I0306 14:45:05.955662   47093 logs.go:124] Gathering logs for storage-provisioner [7fb42f6d4909] ...
I0306 14:45:05.955674   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7fb42f6d4909"
I0306 14:45:06.013799   47093 logs.go:124] Gathering logs for kube-controller-manager [a07448956823] ...
I0306 14:45:06.013811   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 a07448956823"
I0306 14:45:06.135144   47093 logs.go:124] Gathering logs for coredns [1df922a0056b] ...
I0306 14:45:06.135159   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1df922a0056b"
I0306 14:45:06.199792   47093 logs.go:124] Gathering logs for describe nodes ...
I0306 14:45:06.199803   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.26.1/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0306 14:45:06.452627   47093 logs.go:124] Gathering logs for etcd [2cf8cc8ad825] ...
I0306 14:45:06.452638   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2cf8cc8ad825"
I0306 14:45:06.499752   47093 logs.go:124] Gathering logs for kube-controller-manager [87847107d46c] ...
I0306 14:45:06.499763   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 87847107d46c"
I0306 14:45:06.549645   47093 logs.go:124] Gathering logs for Docker ...
I0306 14:45:06.549658   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I0306 14:45:06.616574   47093 logs.go:124] Gathering logs for container status ...
I0306 14:45:06.616585   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0306 14:45:06.700552   47093 logs.go:124] Gathering logs for kubelet ...
I0306 14:45:06.700563   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0306 14:45:06.733350   47093 logs.go:139] Found kubelet problem: Mar 06 10:04:11 minikube kubelet[73131]: W0306 10:04:11.161049   73131 reflector.go:424] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0306 14:45:06.733574   47093 logs.go:139] Found kubelet problem: Mar 06 10:04:11 minikube kubelet[73131]: E0306 10:04:11.161384   73131 reflector.go:140] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0306 14:45:06.783678   47093 out.go:309] Setting ErrFile to fd 2...
I0306 14:45:06.783697   47093 out.go:348] isatty.IsTerminal(2) = true
W0306 14:45:06.783777   47093 out.go:239] ‚ùå  Problems detected in kubelet:
W0306 14:45:06.783791   47093 out.go:239]     Mar 06 10:04:11 minikube kubelet[73131]: W0306 10:04:11.161049   73131 reflector.go:424] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0306 14:45:06.783797   47093 out.go:239]     Mar 06 10:04:11 minikube kubelet[73131]: E0306 10:04:11.161384   73131 reflector.go:140] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0306 14:45:06.783827   47093 out.go:309] Setting ErrFile to fd 2...
I0306 14:45:06.783834   47093 out.go:348] isatty.IsTerminal(2) = true
I0306 14:45:16.784834   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...
I0306 14:45:21.785444   47093 api_server.go:268] stopped: https://10.0.2.15:8443/healthz: Get "https://10.0.2.15:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0306 14:45:22.069052   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I0306 14:45:22.100470   47093 logs.go:279] 2 containers: [642ce8adbf4b 6d337205f63c]
I0306 14:45:22.100548   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I0306 14:45:22.136962   47093 logs.go:279] 2 containers: [2cf8cc8ad825 397454e5053f]
I0306 14:45:22.137036   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I0306 14:45:22.167225   47093 logs.go:279] 4 containers: [11a0413bdd3d 5b83e65e078a 1df922a0056b a510af6aa7e2]
I0306 14:45:22.167301   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I0306 14:45:22.200075   47093 logs.go:279] 2 containers: [623dc1b8e1aa 0abe95ef90b8]
I0306 14:45:22.200145   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I0306 14:45:22.230524   47093 logs.go:279] 2 containers: [d3e945555518 b474fae41d70]
I0306 14:45:22.230602   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kubernetes-dashboard --format={{.ID}}
I0306 14:45:22.269652   47093 logs.go:279] 0 containers: []
W0306 14:45:22.269660   47093 logs.go:281] No container was found matching "kubernetes-dashboard"
I0306 14:45:22.269730   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I0306 14:45:22.302025   47093 logs.go:279] 2 containers: [7fb42f6d4909 bba02e864b5e]
I0306 14:45:22.302093   47093 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I0306 14:45:22.347685   47093 logs.go:279] 2 containers: [87847107d46c a07448956823]
I0306 14:45:22.347699   47093 logs.go:124] Gathering logs for kube-apiserver [6d337205f63c] ...
I0306 14:45:22.347706   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 6d337205f63c"
I0306 14:45:22.419670   47093 logs.go:124] Gathering logs for etcd [2cf8cc8ad825] ...
I0306 14:45:22.419684   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2cf8cc8ad825"
I0306 14:45:22.474764   47093 logs.go:124] Gathering logs for coredns [1df922a0056b] ...
I0306 14:45:22.474776   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 1df922a0056b"
I0306 14:45:22.527377   47093 logs.go:124] Gathering logs for kube-scheduler [623dc1b8e1aa] ...
I0306 14:45:22.527390   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 623dc1b8e1aa"
I0306 14:45:22.601066   47093 logs.go:124] Gathering logs for container status ...
I0306 14:45:22.601079   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I0306 14:45:22.661572   47093 logs.go:124] Gathering logs for describe nodes ...
I0306 14:45:22.661583   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.26.1/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I0306 14:45:22.848647   47093 logs.go:124] Gathering logs for kube-scheduler [0abe95ef90b8] ...
I0306 14:45:22.848658   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 0abe95ef90b8"
I0306 14:45:22.889263   47093 logs.go:124] Gathering logs for kube-proxy [d3e945555518] ...
I0306 14:45:22.889276   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 d3e945555518"
I0306 14:45:22.928869   47093 logs.go:124] Gathering logs for storage-provisioner [bba02e864b5e] ...
I0306 14:45:22.928879   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 bba02e864b5e"
I0306 14:45:22.958754   47093 logs.go:124] Gathering logs for kube-controller-manager [a07448956823] ...
I0306 14:45:22.958764   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 a07448956823"
I0306 14:45:23.007880   47093 logs.go:124] Gathering logs for Docker ...
I0306 14:45:23.007891   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I0306 14:45:23.076498   47093 logs.go:124] Gathering logs for kubelet ...
I0306 14:45:23.076511   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
W0306 14:45:23.112628   47093 logs.go:139] Found kubelet problem: Mar 06 10:04:11 minikube kubelet[73131]: W0306 10:04:11.161049   73131 reflector.go:424] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0306 14:45:23.112763   47093 logs.go:139] Found kubelet problem: Mar 06 10:04:11 minikube kubelet[73131]: E0306 10:04:11.161384   73131 reflector.go:140] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0306 14:45:23.160155   47093 logs.go:124] Gathering logs for etcd [397454e5053f] ...
I0306 14:45:23.160168   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 397454e5053f"
I0306 14:45:23.201902   47093 logs.go:124] Gathering logs for storage-provisioner [7fb42f6d4909] ...
I0306 14:45:23.201912   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7fb42f6d4909"
I0306 14:45:23.237017   47093 logs.go:124] Gathering logs for kube-controller-manager [87847107d46c] ...
I0306 14:45:23.237026   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 87847107d46c"
I0306 14:45:23.295981   47093 logs.go:124] Gathering logs for coredns [a510af6aa7e2] ...
I0306 14:45:23.295993   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 a510af6aa7e2"
I0306 14:45:23.358924   47093 logs.go:124] Gathering logs for kube-proxy [b474fae41d70] ...
I0306 14:45:23.358935   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 b474fae41d70"
I0306 14:45:23.397484   47093 logs.go:124] Gathering logs for dmesg ...
I0306 14:45:23.397494   47093 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0306 14:45:23.426686   47093 logs.go:124] Gathering logs for kube-apiserver [642ce8adbf4b] ...
I0306 14:45:23.426696   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 642ce8adbf4b"
I0306 14:45:23.478367   47093 logs.go:124] Gathering logs for coredns [11a0413bdd3d] ...
I0306 14:45:23.478379   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 11a0413bdd3d"
I0306 14:45:23.517822   47093 logs.go:124] Gathering logs for coredns [5b83e65e078a] ...
I0306 14:45:23.517833   47093 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 5b83e65e078a"
I0306 14:45:23.553384   47093 out.go:309] Setting ErrFile to fd 2...
I0306 14:45:23.553401   47093 out.go:348] isatty.IsTerminal(2) = true
W0306 14:45:23.553487   47093 out.go:239] ‚ùå  Problems detected in kubelet:
W0306 14:45:23.553500   47093 out.go:239]     Mar 06 10:04:11 minikube kubelet[73131]: W0306 10:04:11.161049   73131 reflector.go:424] object-"kube-system"/"coredns": failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
W0306 14:45:23.553505   47093 out.go:239]     Mar 06 10:04:11 minikube kubelet[73131]: E0306 10:04:11.161384   73131 reflector.go:140] object-"kube-system"/"coredns": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "coredns" is forbidden: User "system:node:minikube" cannot list resource "configmaps" in API group "" in the namespace "kube-system": no relationship found between node 'minikube' and this object
I0306 14:45:23.553509   47093 out.go:309] Setting ErrFile to fd 2...
I0306 14:45:23.553514   47093 out.go:348] isatty.IsTerminal(2) = true
I0306 14:45:33.554810   47093 api_server.go:252] Checking apiserver healthz at https://10.0.2.15:8443/healthz ...

* 
* ==> Docker <==
* -- Journal begins at Mon 2023-03-06 09:13:55 UTC, ends at Mon 2023-03-06 13:46:10 UTC. --
Mar 06 13:41:54 minikube dockerd[192810]: time="2023-03-06T13:41:54.258531135Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 06 13:41:54 minikube dockerd[192810]: time="2023-03-06T13:41:54.259336135Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 06 13:41:54 minikube dockerd[192810]: time="2023-03-06T13:41:54.260047136Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 06 13:41:54 minikube dockerd[192810]: time="2023-03-06T13:41:54.260416136Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 06 13:41:54 minikube dockerd[192810]: time="2023-03-06T13:41:54.260968136Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/cf7e3d5e3a82decf633917a1deb10c7653cf0fd2d4d7ddad90d298ce085e305a pid=195481 runtime=io.containerd.runc.v2
Mar 06 13:41:54 minikube dockerd[192810]: time="2023-03-06T13:41:54.261749137Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/44bc2b52f767dac8bbf5c514df27fa6e1646fe8437239601e2f6b7d27ef7931c pid=195488 runtime=io.containerd.runc.v2
Mar 06 13:41:54 minikube dockerd[192810]: time="2023-03-06T13:41:54.276392144Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 06 13:41:54 minikube dockerd[192810]: time="2023-03-06T13:41:54.276893144Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 06 13:41:54 minikube dockerd[192810]: time="2023-03-06T13:41:54.277205145Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 06 13:41:54 minikube dockerd[192810]: time="2023-03-06T13:41:54.282284147Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/e868fd7dba8dca4732dfb0fbe05bba9ec11a9fea932e5bf99ac85793192b36f2 pid=195495 runtime=io.containerd.runc.v2
Mar 06 13:41:55 minikube dockerd[192810]: time="2023-03-06T13:41:55.330720695Z" level=info msg="shim disconnected" id=1df922a0056b7d143ff3fb725d67de148640b85212fd2249c3ff6a52601106c0
Mar 06 13:41:55 minikube dockerd[192810]: time="2023-03-06T13:41:55.330853695Z" level=warning msg="cleaning up after shim disconnected" id=1df922a0056b7d143ff3fb725d67de148640b85212fd2249c3ff6a52601106c0 namespace=moby
Mar 06 13:41:55 minikube dockerd[192810]: time="2023-03-06T13:41:55.330894695Z" level=info msg="cleaning up dead shim"
Mar 06 13:41:55 minikube dockerd[192804]: time="2023-03-06T13:41:55.357913710Z" level=info msg="ignoring event" container=1df922a0056b7d143ff3fb725d67de148640b85212fd2249c3ff6a52601106c0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 06 13:41:55 minikube dockerd[192810]: time="2023-03-06T13:41:55.841190973Z" level=warning msg="cleanup warnings time=\"2023-03-06T13:41:55Z\" level=info msg=\"starting signal loop\" namespace=moby pid=195585 runtime=io.containerd.runc.v2\n"
Mar 06 13:41:55 minikube dockerd[192810]: time="2023-03-06T13:41:55.981455049Z" level=info msg="shim disconnected" id=a510af6aa7e2bf8246706b0618970fed74a22841d61e389f7255dedba126de8c
Mar 06 13:41:55 minikube dockerd[192810]: time="2023-03-06T13:41:55.981779049Z" level=warning msg="cleaning up after shim disconnected" id=a510af6aa7e2bf8246706b0618970fed74a22841d61e389f7255dedba126de8c namespace=moby
Mar 06 13:41:55 minikube dockerd[192810]: time="2023-03-06T13:41:55.982016049Z" level=info msg="cleaning up dead shim"
Mar 06 13:41:56 minikube dockerd[192804]: time="2023-03-06T13:41:56.027030075Z" level=info msg="ignoring event" container=a510af6aa7e2bf8246706b0618970fed74a22841d61e389f7255dedba126de8c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 06 13:41:56 minikube dockerd[192810]: time="2023-03-06T13:41:56.031713077Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 06 13:41:56 minikube dockerd[192810]: time="2023-03-06T13:41:56.033825078Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 06 13:41:56 minikube dockerd[192810]: time="2023-03-06T13:41:56.034484079Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 06 13:41:56 minikube dockerd[192810]: time="2023-03-06T13:41:56.036692080Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/642ce8adbf4b9ef3fca106a8abbc7f94991bad38f9b036a1d914b40638e5c849 pid=195656 runtime=io.containerd.runc.v2
Mar 06 13:41:56 minikube dockerd[192810]: time="2023-03-06T13:41:56.089642111Z" level=warning msg="cleanup warnings time=\"2023-03-06T13:41:56Z\" level=info msg=\"starting signal loop\" namespace=moby pid=195678 runtime=io.containerd.runc.v2\n"
Mar 06 13:41:57 minikube dockerd[192810]: time="2023-03-06T13:41:57.213984765Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 06 13:41:57 minikube dockerd[192810]: time="2023-03-06T13:41:57.214103765Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 06 13:41:57 minikube dockerd[192810]: time="2023-03-06T13:41:57.214145765Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 06 13:41:57 minikube dockerd[192810]: time="2023-03-06T13:41:57.219497769Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/d55abaa33ea4a58b546a38984dcdc6cd563c00bdc4d610313c0afacf351dd4d9 pid=195864 runtime=io.containerd.runc.v2
Mar 06 13:42:02 minikube dockerd[192810]: time="2023-03-06T13:42:02.610034461Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 06 13:42:02 minikube dockerd[192810]: time="2023-03-06T13:42:02.610770462Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 06 13:42:02 minikube dockerd[192810]: time="2023-03-06T13:42:02.610824462Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 06 13:42:02 minikube dockerd[192810]: time="2023-03-06T13:42:02.612714463Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/87847107d46c264fa2780195a1a37c6c9fe0cb8288d66bf5bc6163e855d5bea7 pid=196146 runtime=io.containerd.runc.v2
Mar 06 13:42:02 minikube dockerd[192810]: time="2023-03-06T13:42:02.836422635Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 06 13:42:02 minikube dockerd[192810]: time="2023-03-06T13:42:02.868598660Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 06 13:42:02 minikube dockerd[192810]: time="2023-03-06T13:42:02.868694660Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 06 13:42:02 minikube dockerd[192810]: time="2023-03-06T13:42:02.880444669Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/623dc1b8e1aa0c71e6ca1905fd6a45b997322a6b35d6ef03a4c66cf3b6687419 pid=196178 runtime=io.containerd.runc.v2
Mar 06 13:42:03 minikube dockerd[192810]: time="2023-03-06T13:42:03.001592762Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 06 13:42:03 minikube dockerd[192810]: time="2023-03-06T13:42:03.002141762Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 06 13:42:03 minikube dockerd[192810]: time="2023-03-06T13:42:03.002183762Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 06 13:42:03 minikube dockerd[192810]: time="2023-03-06T13:42:03.016177773Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/2cf8cc8ad8253d4b330cc101ade9ffe84ea644dd7bba39c362137c81fe00a0fa pid=196202 runtime=io.containerd.runc.v2
Mar 06 13:42:13 minikube dockerd[192810]: time="2023-03-06T13:42:13.139893309Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 06 13:42:13 minikube dockerd[192810]: time="2023-03-06T13:42:13.139989309Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 06 13:42:13 minikube dockerd[192810]: time="2023-03-06T13:42:13.140038309Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 06 13:42:13 minikube dockerd[192810]: time="2023-03-06T13:42:13.140266309Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/7fb42f6d4909dda900f50d1b2d0c15b71cea54af00d078ad3eb54285ab2b1308 pid=196477 runtime=io.containerd.runc.v2
Mar 06 13:42:13 minikube dockerd[192810]: time="2023-03-06T13:42:13.816240060Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 06 13:42:13 minikube dockerd[192810]: time="2023-03-06T13:42:13.816558060Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 06 13:42:13 minikube dockerd[192810]: time="2023-03-06T13:42:13.816664060Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 06 13:42:13 minikube dockerd[192810]: time="2023-03-06T13:42:13.837530083Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/10942a691b257f6f2dc8bd751e8c0752db53ecf57e7f5087a6345edeedcfb8a8 pid=196513 runtime=io.containerd.runc.v2
Mar 06 13:42:14 minikube dockerd[192810]: time="2023-03-06T13:42:14.794473170Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 06 13:42:14 minikube dockerd[192810]: time="2023-03-06T13:42:14.794708171Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 06 13:42:14 minikube dockerd[192810]: time="2023-03-06T13:42:14.795034171Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 06 13:42:14 minikube dockerd[192810]: time="2023-03-06T13:42:14.797417174Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/5b83e65e078a2d1b35fa8d2b091022f7cf76be875b41e3b9ef69641aff6a6441 pid=196581 runtime=io.containerd.runc.v2
Mar 06 13:42:14 minikube dockerd[192810]: time="2023-03-06T13:42:14.826855207Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 06 13:42:14 minikube dockerd[192810]: time="2023-03-06T13:42:14.827102208Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 06 13:42:14 minikube dockerd[192810]: time="2023-03-06T13:42:14.827196208Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 06 13:42:14 minikube dockerd[192810]: time="2023-03-06T13:42:14.829376210Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/d3e945555518a7ff2654275a002cd47c606a0c11b4265ac8fee89a8b67270a5f pid=196600 runtime=io.containerd.runc.v2
Mar 06 13:42:20 minikube dockerd[192810]: time="2023-03-06T13:42:20.345763030Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 06 13:42:20 minikube dockerd[192810]: time="2023-03-06T13:42:20.346334030Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 06 13:42:20 minikube dockerd[192810]: time="2023-03-06T13:42:20.347201032Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 06 13:42:20 minikube dockerd[192810]: time="2023-03-06T13:42:20.347711032Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/11a0413bdd3d9c1906c8e72fa2c9d06057d6b226524c4ae2569a5a09d3eaa3cd pid=196793 runtime=io.containerd.runc.v2

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID
11a0413bdd3d9       5185b96f0becf       3 minutes ago       Running             coredns                   2                   10942a691b257
5b83e65e078a2       5185b96f0becf       3 minutes ago       Running             coredns                   2                   d55abaa33ea4a
d3e945555518a       46a6bb3c77ce0       3 minutes ago       Running             kube-proxy                2                   e868fd7dba8dc
7fb42f6d4909d       6e38f40d628db       3 minutes ago       Running             storage-provisioner       2                   cf7e3d5e3a82d
2cf8cc8ad8253       fce326961ae2d       4 minutes ago       Running             etcd                      3                   4310a69025515
623dc1b8e1aa0       655493523f607       4 minutes ago       Running             kube-scheduler            3                   34eb3bc62bbf9
87847107d46c2       e9c08e11b07f6       4 minutes ago       Running             kube-controller-manager   2                   da43800d879e5
642ce8adbf4b9       deb04688c4a35       4 minutes ago       Running             kube-apiserver            2                   44bc2b52f767d
b474fae41d704       46a6bb3c77ce0       4 minutes ago       Exited              kube-proxy                1                   496ee56836928
bba02e864b5e2       6e38f40d628db       4 minutes ago       Exited              storage-provisioner       1                   ad151dd8150ff
1df922a0056b7       5185b96f0becf       4 minutes ago       Exited              coredns                   1                   420c7c4f3ddc8
0abe95ef90b87       655493523f607       4 minutes ago       Exited              kube-scheduler            2                   5141b13d4a47c
397454e5053f0       fce326961ae2d       4 minutes ago       Exited              etcd                      2                   b877d3c7f5036
a510af6aa7e2b       5185b96f0becf       4 minutes ago       Exited              coredns                   1                   4b8921c13a3bd
6d337205f63cf       deb04688c4a35       4 minutes ago       Exited              kube-apiserver            1                   0761582fbdb44
a07448956823d       e9c08e11b07f6       4 minutes ago       Exited              kube-controller-manager   1                   61c52f61d76a8

* 
* ==> coredns [11a0413bdd3d] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = 4369d49e705690634e66dc4876ba448687add67b4e702a1c8bd9cbe26bf81de42209d08c6b52f2167c69004abbe79b233480d7bb5830c218d455f30e7efd3686
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[INFO] 127.0.0.1:39838 - 51431 "HINFO IN 4750831466316368.1796508836618132437. udp 54 false 512" NXDOMAIN qr,rd,ra 129 0.025249034s

* 
* ==> coredns [1df922a0056b] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 4369d49e705690634e66dc4876ba448687add67b4e702a1c8bd9cbe26bf81de42209d08c6b52f2167c69004abbe79b233480d7bb5830c218d455f30e7efd3686
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[INFO] 127.0.0.1:56600 - 62704 "HINFO IN 376416457995544324.2718753221123942737. udp 56 false 512" NXDOMAIN qr,rd,ra 131 0.018128993s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [5b83e65e078a] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = 4369d49e705690634e66dc4876ba448687add67b4e702a1c8bd9cbe26bf81de42209d08c6b52f2167c69004abbe79b233480d7bb5830c218d455f30e7efd3686
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[INFO] 127.0.0.1:41276 - 28719 "HINFO IN 8081051569814531618.1382846177515893844. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.01749402s

* 
* ==> coredns [a510af6aa7e2] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 4369d49e705690634e66dc4876ba448687add67b4e702a1c8bd9cbe26bf81de42209d08c6b52f2167c69004abbe79b233480d7bb5830c218d455f30e7efd3686
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[INFO] 127.0.0.1:33706 - 58005 "HINFO IN 6175630835237800897.2770829055462701219. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.050657979s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=ddac20b4b34a9c8c857fc602203b6ba2679794d3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_03_06T11_03_56_0700
                    minikube.k8s.io/version=v1.29.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 06 Mar 2023 10:03:50 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 06 Mar 2023 13:46:07 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 06 Mar 2023 13:42:11 +0000   Mon, 06 Mar 2023 10:03:48 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 06 Mar 2023 13:42:11 +0000   Mon, 06 Mar 2023 10:03:48 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 06 Mar 2023 13:42:11 +0000   Mon, 06 Mar 2023 10:03:48 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 06 Mar 2023 13:42:11 +0000   Mon, 06 Mar 2023 10:04:07 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  10.0.2.15
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  17784760Ki
  hugepages-2Mi:      0
  memory:             3914528Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  17784760Ki
  hugepages-2Mi:      0
  memory:             3914528Ki
  pods:               110
System Info:
  Machine ID:                 dbf75bd09d2c4c438c33c2ed511e1e3c
  System UUID:                dbf75bd09d2c4c438c33c2ed511e1e3c
  Boot ID:                    67601d77-de57-46c4-8d1a-dfe786dd3125
  Kernel Version:             5.10.57
  OS Image:                   Buildroot 2021.02.12
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.23
  Kubelet Version:            v1.26.1
  Kube-Proxy Version:         v1.26.1
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-787d4945fb-9jkj4            100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     3h42m
  kube-system                 coredns-787d4945fb-f2hdp            100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     3h42m
  kube-system                 etcd-minikube                       100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         3h42m
  kube-system                 kube-apiserver-minikube             250m (12%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3h42m
  kube-system                 kube-controller-manager-minikube    200m (10%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3h42m
  kube-system                 kube-proxy-wtqlm                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3h42m
  kube-system                 kube-scheduler-minikube             100m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3h42m
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3h42m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (42%!)(MISSING)  0 (0%!)(MISSING)
  memory             240Mi (6%!)(MISSING)  340Mi (8%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                   From             Message
  ----    ------                   ----                  ----             -------
  Normal  Starting                 4m41s                 kube-proxy       
  Normal  Starting                 3m55s                 kube-proxy       
  Normal  RegisteredNode           4m28s                 node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 4m10s                 kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  4m10s                 kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  4m9s (x8 over 4m10s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    4m9s (x8 over 4m10s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     4m9s (x7 over 4m10s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           3m40s                 node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [ +10.503558] clocksource: timekeeping watchdog on CPU0: hpet retried 2 times before success
[Mar 6 09:34] clocksource: timekeeping watchdog on CPU0: hpet retried 2 times before success
[  +2.999627] clocksource: timekeeping watchdog on CPU0: hpet retried 2 times before success
[Mar 6 09:35] clocksource: timekeeping watchdog on CPU1: hpet retried 3 times before success
[  +2.000634] clocksource: timekeeping watchdog on CPU1: hpet retried 2 times before success
[  +6.999953] clocksource: timekeeping watchdog on CPU1: hpet retried 2 times before success
[Mar 6 09:36] clocksource: timekeeping watchdog on CPU1: hpet retried 2 times before success
[  +6.000124] clocksource: timekeeping watchdog on CPU1: hpet retried 3 times before success
[Mar 6 09:37] clocksource: timekeeping watchdog on CPU0: hpet retried 3 times before success
[ +19.495757] clocksource: timekeeping watchdog on CPU1: hpet retried 2 times before success
[Mar 6 09:38] clocksource: timekeeping watchdog on CPU1: hpet retried 2 times before success
[ +38.997822] clocksource: timekeeping watchdog on CPU1: hpet retried 2 times before success
[Mar 6 09:39] clocksource: timekeeping watchdog on CPU1: hpet retried 2 times before success
[  +2.000144] clocksource: timekeeping watchdog on CPU1: hpet retried 2 times before success
[  +3.999211] clocksource: timekeeping watchdog on CPU1: hpet read-back delay of 113000ns, attempt 4, marking unstable
[  +0.002844] TSC found unstable after boot, most likely due to broken BIOS. Use 'tsc=unstable'.
[  +0.000994] clocksource: Checking clocksource tsc synchronization from CPU 1.
[Mar 6 09:44] systemd-fstab-generator[39022]: Ignoring "noauto" for root device
[  +0.426721] systemd-fstab-generator[39055]: Ignoring "noauto" for root device
[  +0.254611] systemd-fstab-generator[39066]: Ignoring "noauto" for root device
[  +0.228867] systemd-fstab-generator[39079]: Ignoring "noauto" for root device
[  +7.834421] systemd-fstab-generator[40034]: Ignoring "noauto" for root device
[  +0.214440] systemd-fstab-generator[40045]: Ignoring "noauto" for root device
[  +0.156787] systemd-fstab-generator[40056]: Ignoring "noauto" for root device
[  +0.172747] systemd-fstab-generator[40073]: Ignoring "noauto" for root device
[  +2.489548] kauditd_printk_skb: 35 callbacks suppressed
[ +16.443552] kauditd_printk_skb: 12 callbacks suppressed
[Mar 6 09:45] systemd-fstab-generator[42725]: Ignoring "noauto" for root device
[ +23.466314] kauditd_printk_skb: 4 callbacks suppressed
[Mar 6 09:49] systemd-fstab-generator[51817]: Ignoring "noauto" for root device
[ +13.067358] systemd-fstab-generator[52499]: Ignoring "noauto" for root device
[ +14.555238] kauditd_printk_skb: 38 callbacks suppressed
[Mar 6 09:58] systemd-fstab-generator[60117]: Ignoring "noauto" for root device
[  +0.426673] systemd-fstab-generator[60150]: Ignoring "noauto" for root device
[  +0.216076] systemd-fstab-generator[60161]: Ignoring "noauto" for root device
[  +0.253078] systemd-fstab-generator[60174]: Ignoring "noauto" for root device
[  +5.365372] kauditd_printk_skb: 8 callbacks suppressed
[ +14.076849] kauditd_printk_skb: 5 callbacks suppressed
[  +0.877577] systemd-fstab-generator[61298]: Ignoring "noauto" for root device
[  +0.240392] systemd-fstab-generator[61345]: Ignoring "noauto" for root device
[  +0.236483] systemd-fstab-generator[61396]: Ignoring "noauto" for root device
[  +0.249554] systemd-fstab-generator[61460]: Ignoring "noauto" for root device
[  +5.632981] kauditd_printk_skb: 54 callbacks suppressed
[Mar 6 09:59] systemd-fstab-generator[64047]: Ignoring "noauto" for root device
[Mar 6 10:03] systemd-fstab-generator[72436]: Ignoring "noauto" for root device
[ +14.622927] systemd-fstab-generator[73125]: Ignoring "noauto" for root device
[Mar 6 10:04] kauditd_printk_skb: 38 callbacks suppressed
[Mar 6 11:04] kauditd_printk_skb: 8 callbacks suppressed
[Mar 6 13:40] systemd-fstab-generator[192006]: Ignoring "noauto" for root device
[  +0.615439] systemd-fstab-generator[192042]: Ignoring "noauto" for root device
[  +0.251032] systemd-fstab-generator[192053]: Ignoring "noauto" for root device
[  +0.324520] systemd-fstab-generator[192066]: Ignoring "noauto" for root device
[Mar 6 13:41] systemd-fstab-generator[193107]: Ignoring "noauto" for root device
[  +0.461338] systemd-fstab-generator[193124]: Ignoring "noauto" for root device
[  +0.374577] systemd-fstab-generator[193142]: Ignoring "noauto" for root device
[  +0.319168] systemd-fstab-generator[193182]: Ignoring "noauto" for root device
[  +3.065868] kauditd_printk_skb: 37 callbacks suppressed
[  +7.334625] kauditd_printk_skb: 6 callbacks suppressed
[Mar 6 13:42] systemd-fstab-generator[195984]: Ignoring "noauto" for root device
[ +15.030544] kauditd_printk_skb: 5 callbacks suppressed

* 
* ==> etcd [2cf8cc8ad825] <==
* {"level":"info","ts":"2023-03-06T13:42:04.762Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2023-03-06T13:42:04.762Z","caller":"embed/etcd.go:124","msg":"configuring peer listeners","listen-peer-urls":["https://10.0.2.15:2380"]}
{"level":"info","ts":"2023-03-06T13:42:04.762Z","caller":"embed/etcd.go:484","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-03-06T13:42:04.765Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://10.0.2.15:2379","https://127.0.0.1:2379"]}
{"level":"info","ts":"2023-03-06T13:42:04.766Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.6","git-sha":"cecbe35ce","go-version":"go1.16.15","go-os":"linux","go-arch":"amd64","max-cpu-set":2,"max-cpu-available":2,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://10.0.2.15:2380"],"listen-peer-urls":["https://10.0.2.15:2380"],"advertise-client-urls":["https://10.0.2.15:2379"],"listen-client-urls":["https://10.0.2.15:2379","https://127.0.0.1:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-03-06T13:42:04.771Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"5.558004ms"}
{"level":"info","ts":"2023-03-06T13:42:05.214Z","caller":"etcdserver/server.go:509","msg":"recovered v2 store from snapshot","snapshot-index":10001,"snapshot-size":"7.1 kB"}
{"level":"info","ts":"2023-03-06T13:42:05.214Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":1413120,"backend-size":"1.4 MB","backend-size-in-use-bytes":995328,"backend-size-in-use":"995 kB"}
{"level":"info","ts":"2023-03-06T13:42:05.421Z","caller":"etcdserver/raft.go:529","msg":"restarting local member","cluster-id":"ef296cf39f5d9d66","local-member-id":"f074a195de705325","commit-index":13523}
{"level":"info","ts":"2023-03-06T13:42:05.422Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 switched to configuration voters=(17326651331455243045)"}
{"level":"info","ts":"2023-03-06T13:42:05.422Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 became follower at term 4"}
{"level":"info","ts":"2023-03-06T13:42:05.422Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft f074a195de705325 [peers: [f074a195de705325], term: 4, commit: 13523, applied: 10001, lastindex: 13523, lastterm: 4]"}
{"level":"info","ts":"2023-03-06T13:42:05.422Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-03-06T13:42:05.422Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"ef296cf39f5d9d66","local-member-id":"f074a195de705325","recovered-remote-peer-id":"f074a195de705325","recovered-remote-peer-urls":["https://10.0.2.15:2380"]}
{"level":"info","ts":"2023-03-06T13:42:05.422Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2023-03-06T13:42:05.431Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-03-06T13:42:05.433Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":10386}
{"level":"info","ts":"2023-03-06T13:42:05.436Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":10852}
{"level":"info","ts":"2023-03-06T13:42:05.444Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-03-06T13:42:05.459Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"f074a195de705325","timeout":"7s"}
{"level":"info","ts":"2023-03-06T13:42:05.463Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"f074a195de705325"}
{"level":"info","ts":"2023-03-06T13:42:05.463Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"f074a195de705325","local-server-version":"3.5.6","cluster-id":"ef296cf39f5d9d66","cluster-version":"3.5"}
{"level":"info","ts":"2023-03-06T13:42:05.485Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-03-06T13:42:05.488Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"f074a195de705325","initial-advertise-peer-urls":["https://10.0.2.15:2380"],"listen-peer-urls":["https://10.0.2.15:2380"],"advertise-client-urls":["https://10.0.2.15:2379"],"listen-client-urls":["https://10.0.2.15:2379","https://127.0.0.1:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-03-06T13:42:05.488Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-03-06T13:42:05.488Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"f074a195de705325","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2023-03-06T13:42:05.498Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"10.0.2.15:2380"}
{"level":"info","ts":"2023-03-06T13:42:05.498Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"10.0.2.15:2380"}
{"level":"info","ts":"2023-03-06T13:42:05.498Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2023-03-06T13:42:05.498Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2023-03-06T13:42:05.498Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2023-03-06T13:42:06.082Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 is starting a new election at term 4"}
{"level":"info","ts":"2023-03-06T13:42:06.082Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 became pre-candidate at term 4"}
{"level":"info","ts":"2023-03-06T13:42:06.082Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 received MsgPreVoteResp from f074a195de705325 at term 4"}
{"level":"info","ts":"2023-03-06T13:42:06.082Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 became candidate at term 5"}
{"level":"info","ts":"2023-03-06T13:42:06.082Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 received MsgVoteResp from f074a195de705325 at term 5"}
{"level":"info","ts":"2023-03-06T13:42:06.082Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 became leader at term 5"}
{"level":"info","ts":"2023-03-06T13:42:06.082Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: f074a195de705325 elected leader f074a195de705325 at term 5"}
{"level":"warn","ts":"2023-03-06T13:42:06.089Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"169.619149ms","expected-duration":"100ms","prefix":"","request":"header:<ID:5991342997148400565 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:8785 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<>>","response":""}
{"level":"info","ts":"2023-03-06T13:42:07.029Z","caller":"etcdserver/server.go:2054","msg":"published local member to cluster through raft","local-member-id":"f074a195de705325","local-member-attributes":"{Name:minikube ClientURLs:[https://10.0.2.15:2379]}","request-path":"/0/members/f074a195de705325/attributes","cluster-id":"ef296cf39f5d9d66","publish-timeout":"7s"}
{"level":"info","ts":"2023-03-06T13:42:07.030Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-03-06T13:42:07.030Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-03-06T13:42:07.033Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"10.0.2.15:2379"}
{"level":"info","ts":"2023-03-06T13:42:07.035Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-03-06T13:42:07.035Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-03-06T13:42:07.037Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-03-06T13:42:07.874Z","caller":"traceutil/trace.go:171","msg":"trace[1452236203] linearizableReadLoop","detail":"{readStateIndex:13525; appliedIndex:13525; }","duration":"198.988184ms","start":"2023-03-06T13:42:07.675Z","end":"2023-03-06T13:42:07.874Z","steps":["trace[1452236203] 'read index received'  (duration: 198.911184ms)","trace[1452236203] 'applied index is now lower than readState.Index'  (duration: 36¬µs)"],"step_count":2}
{"level":"warn","ts":"2023-03-06T13:42:07.875Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"217.074201ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/\" range_end:\"/registry/namespaces0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2023-03-06T13:42:07.875Z","caller":"traceutil/trace.go:171","msg":"trace[419246115] range","detail":"{range_begin:/registry/namespaces/; range_end:/registry/namespaces0; response_count:0; response_revision:10852; }","duration":"217.325201ms","start":"2023-03-06T13:42:07.657Z","end":"2023-03-06T13:42:07.875Z","steps":["trace[419246115] 'agreement among raft nodes before linearized reading'  (duration: 216.3972ms)"],"step_count":1}
{"level":"info","ts":"2023-03-06T13:42:16.435Z","caller":"traceutil/trace.go:171","msg":"trace[810914297] linearizableReadLoop","detail":"{readStateIndex:13578; appliedIndex:13577; }","duration":"106.857129ms","start":"2023-03-06T13:42:16.328Z","end":"2023-03-06T13:42:16.435Z","steps":["trace[810914297] 'read index received'  (duration: 106.303128ms)","trace[810914297] 'applied index is now lower than readState.Index'  (duration: 499.001¬µs)"],"step_count":2}
{"level":"warn","ts":"2023-03-06T13:42:16.435Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"107.355129ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/generic-garbage-collector\" ","response":"range_response_count:1 size:216"}
{"level":"info","ts":"2023-03-06T13:42:16.436Z","caller":"traceutil/trace.go:171","msg":"trace[1930517402] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/generic-garbage-collector; range_end:; response_count:1; response_revision:10901; }","duration":"107.48913ms","start":"2023-03-06T13:42:16.328Z","end":"2023-03-06T13:42:16.436Z","steps":["trace[1930517402] 'agreement among raft nodes before linearized reading'  (duration: 107.112129ms)"],"step_count":1}
{"level":"info","ts":"2023-03-06T13:42:17.082Z","caller":"traceutil/trace.go:171","msg":"trace[1181640027] transaction","detail":"{read_only:false; response_revision:10903; number_of_response:1; }","duration":"153.788186ms","start":"2023-03-06T13:42:16.927Z","end":"2023-03-06T13:42:17.081Z","steps":["trace[1181640027] 'process raft request'  (duration: 148.84018ms)"],"step_count":1}
{"level":"info","ts":"2023-03-06T13:42:17.759Z","caller":"traceutil/trace.go:171","msg":"trace[604387875] transaction","detail":"{read_only:false; response_revision:10905; number_of_response:1; }","duration":"149.274184ms","start":"2023-03-06T13:42:17.610Z","end":"2023-03-06T13:42:17.759Z","steps":["trace[604387875] 'process raft request'  (duration: 148.420183ms)"],"step_count":1}
{"level":"info","ts":"2023-03-06T13:42:17.889Z","caller":"traceutil/trace.go:171","msg":"trace[280904470] linearizableReadLoop","detail":"{readStateIndex:13582; appliedIndex:13582; }","duration":"136.395168ms","start":"2023-03-06T13:42:17.753Z","end":"2023-03-06T13:42:17.889Z","steps":["trace[280904470] 'read index received'  (duration: 136.315168ms)","trace[280904470] 'applied index is now lower than readState.Index'  (duration: 38¬µs)"],"step_count":2}
{"level":"warn","ts":"2023-03-06T13:42:17.891Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"137.68617ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/bootstrap-signer\" ","response":"range_response_count:1 size:197"}
{"level":"info","ts":"2023-03-06T13:42:17.891Z","caller":"traceutil/trace.go:171","msg":"trace[1790431595] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/bootstrap-signer; range_end:; response_count:1; response_revision:10905; }","duration":"137.920171ms","start":"2023-03-06T13:42:17.753Z","end":"2023-03-06T13:42:17.891Z","steps":["trace[1790431595] 'agreement among raft nodes before linearized reading'  (duration: 136.653169ms)"],"step_count":1}
{"level":"info","ts":"2023-03-06T13:43:14.988Z","caller":"traceutil/trace.go:171","msg":"trace[1807875043] transaction","detail":"{read_only:false; response_revision:10973; number_of_response:1; }","duration":"169.785491ms","start":"2023-03-06T13:43:14.818Z","end":"2023-03-06T13:43:14.988Z","steps":["trace[1807875043] 'process raft request'  (duration: 108.168313ms)","trace[1807875043] 'compare'  (duration: 10.263029ms)","trace[1807875043] 'attach lease to kv pair' {req_type:put; key:/registry/masterleases/10.0.2.15; req_size:108; } (duration: 42.173122ms)"],"step_count":3}
{"level":"warn","ts":"2023-03-06T13:43:15.168Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"103.669302ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/\" range_end:\"/registry/masterleases0\" ","response":"range_response_count:1 size:125"}
{"level":"info","ts":"2023-03-06T13:43:15.168Z","caller":"traceutil/trace.go:171","msg":"trace[325572223] range","detail":"{range_begin:/registry/masterleases/; range_end:/registry/masterleases0; response_count:1; response_revision:10973; }","duration":"104.092303ms","start":"2023-03-06T13:43:15.064Z","end":"2023-03-06T13:43:15.168Z","steps":["trace[325572223] 'range keys from in-memory index tree'  (duration: 97.976285ms)"],"step_count":1}

* 
* ==> etcd [397454e5053f] <==
* {"level":"info","ts":"2023-03-06T13:41:22.851Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://10.0.2.15:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://10.0.2.15:2380","--initial-cluster=minikube=https://10.0.2.15:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://10.0.2.15:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://10.0.2.15:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2023-03-06T13:41:22.860Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2023-03-06T13:41:22.860Z","caller":"embed/etcd.go:124","msg":"configuring peer listeners","listen-peer-urls":["https://10.0.2.15:2380"]}
{"level":"info","ts":"2023-03-06T13:41:22.860Z","caller":"embed/etcd.go:484","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-03-06T13:41:22.861Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://10.0.2.15:2379","https://127.0.0.1:2379"]}
{"level":"info","ts":"2023-03-06T13:41:22.862Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.6","git-sha":"cecbe35ce","go-version":"go1.16.15","go-os":"linux","go-arch":"amd64","max-cpu-set":2,"max-cpu-available":2,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://10.0.2.15:2380"],"listen-peer-urls":["https://10.0.2.15:2380"],"advertise-client-urls":["https://10.0.2.15:2379"],"listen-client-urls":["https://10.0.2.15:2379","https://127.0.0.1:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-03-06T13:41:22.868Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"889¬µs"}
{"level":"info","ts":"2023-03-06T13:41:23.007Z","caller":"etcdserver/server.go:509","msg":"recovered v2 store from snapshot","snapshot-index":10001,"snapshot-size":"7.1 kB"}
{"level":"info","ts":"2023-03-06T13:41:23.007Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":1413120,"backend-size":"1.4 MB","backend-size-in-use-bytes":692224,"backend-size-in-use":"692 kB"}
{"level":"info","ts":"2023-03-06T13:41:23.344Z","caller":"etcdserver/raft.go:529","msg":"restarting local member","cluster-id":"ef296cf39f5d9d66","local-member-id":"f074a195de705325","commit-index":13386}
{"level":"info","ts":"2023-03-06T13:41:23.344Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 switched to configuration voters=(17326651331455243045)"}
{"level":"info","ts":"2023-03-06T13:41:23.344Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 became follower at term 3"}
{"level":"info","ts":"2023-03-06T13:41:23.344Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft f074a195de705325 [peers: [f074a195de705325], term: 3, commit: 13386, applied: 10001, lastindex: 13386, lastterm: 3]"}
{"level":"info","ts":"2023-03-06T13:41:23.345Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-03-06T13:41:23.345Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"ef296cf39f5d9d66","local-member-id":"f074a195de705325","recovered-remote-peer-id":"f074a195de705325","recovered-remote-peer-urls":["https://10.0.2.15:2380"]}
{"level":"info","ts":"2023-03-06T13:41:23.345Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2023-03-06T13:41:23.368Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-03-06T13:41:23.376Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":10386}
{"level":"info","ts":"2023-03-06T13:41:23.390Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":10723}
{"level":"info","ts":"2023-03-06T13:41:23.408Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-03-06T13:41:23.451Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"f074a195de705325","timeout":"7s"}
{"level":"info","ts":"2023-03-06T13:41:23.452Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"f074a195de705325"}
{"level":"info","ts":"2023-03-06T13:41:23.452Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"f074a195de705325","local-server-version":"3.5.6","cluster-id":"ef296cf39f5d9d66","cluster-version":"3.5"}
{"level":"info","ts":"2023-03-06T13:41:23.455Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-03-06T13:41:23.468Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"f074a195de705325","initial-advertise-peer-urls":["https://10.0.2.15:2380"],"listen-peer-urls":["https://10.0.2.15:2380"],"advertise-client-urls":["https://10.0.2.15:2379"],"listen-client-urls":["https://10.0.2.15:2379","https://127.0.0.1:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-03-06T13:41:23.468Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-03-06T13:41:23.468Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"f074a195de705325","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2023-03-06T13:41:23.468Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2023-03-06T13:41:23.468Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2023-03-06T13:41:23.468Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2023-03-06T13:41:23.469Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"10.0.2.15:2380"}
{"level":"info","ts":"2023-03-06T13:41:23.469Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"10.0.2.15:2380"}
{"level":"info","ts":"2023-03-06T13:41:23.546Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 is starting a new election at term 3"}
{"level":"info","ts":"2023-03-06T13:41:23.546Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 became pre-candidate at term 3"}
{"level":"info","ts":"2023-03-06T13:41:23.546Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 received MsgPreVoteResp from f074a195de705325 at term 3"}
{"level":"info","ts":"2023-03-06T13:41:23.546Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 became candidate at term 4"}
{"level":"info","ts":"2023-03-06T13:41:23.547Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 received MsgVoteResp from f074a195de705325 at term 4"}
{"level":"info","ts":"2023-03-06T13:41:23.547Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"f074a195de705325 became leader at term 4"}
{"level":"info","ts":"2023-03-06T13:41:23.547Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: f074a195de705325 elected leader f074a195de705325 at term 4"}
{"level":"info","ts":"2023-03-06T13:41:23.899Z","caller":"etcdserver/server.go:2054","msg":"published local member to cluster through raft","local-member-id":"f074a195de705325","local-member-attributes":"{Name:minikube ClientURLs:[https://10.0.2.15:2379]}","request-path":"/0/members/f074a195de705325/attributes","cluster-id":"ef296cf39f5d9d66","publish-timeout":"7s"}
{"level":"info","ts":"2023-03-06T13:41:23.899Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-03-06T13:41:23.902Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-03-06T13:41:23.904Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-03-06T13:41:23.904Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"10.0.2.15:2379"}
{"level":"info","ts":"2023-03-06T13:41:23.915Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2023-03-06T13:41:23.915Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-03-06T13:41:50.486Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2023-03-06T13:41:50.486Z","caller":"embed/etcd.go:373","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://10.0.2.15:2380"],"advertise-client-urls":["https://10.0.2.15:2379"]}
{"level":"info","ts":"2023-03-06T13:41:50.688Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"f074a195de705325","current-leader-member-id":"f074a195de705325"}
{"level":"info","ts":"2023-03-06T13:41:50.732Z","caller":"embed/etcd.go:568","msg":"stopping serving peer traffic","address":"10.0.2.15:2380"}
{"level":"info","ts":"2023-03-06T13:41:50.736Z","caller":"embed/etcd.go:573","msg":"stopped serving peer traffic","address":"10.0.2.15:2380"}
{"level":"info","ts":"2023-03-06T13:41:50.736Z","caller":"embed/etcd.go:375","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://10.0.2.15:2380"],"advertise-client-urls":["https://10.0.2.15:2379"]}

* 
* ==> kernel <==
*  13:46:11 up  4:32,  0 users,  load average: 0.99, 2.23, 1.62
Linux minikube 5.10.57 #1 SMP Fri Jan 27 18:05:35 UTC 2023 x86_64 GNU/Linux
PRETTY_NAME="Buildroot 2021.02.12"

* 
* ==> kube-apiserver [642ce8adbf4b] <==
* I0306 13:42:10.633876       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0306 13:42:10.634550       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0306 13:42:10.634945       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0306 13:42:10.643147       1 autoregister_controller.go:141] Starting autoregister controller
I0306 13:42:10.643533       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0306 13:42:10.644273       1 customresource_discovery_controller.go:288] Starting DiscoveryController
I0306 13:42:10.645266       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0306 13:42:10.647194       1 controller.go:83] Starting OpenAPI AggregationController
I0306 13:42:10.648420       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0306 13:42:10.648827       1 apf_controller.go:361] Starting API Priority and Fairness config controller
I0306 13:42:10.649546       1 available_controller.go:494] Starting AvailableConditionController
I0306 13:42:10.649838       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0306 13:42:10.650326       1 controller.go:121] Starting legacy_token_tracking_controller
I0306 13:42:10.650904       1 shared_informer.go:273] Waiting for caches to sync for configmaps
I0306 13:42:10.651420       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0306 13:42:10.653834       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0306 13:42:10.654168       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0306 13:42:10.654997       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0306 13:42:10.728288       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0306 13:42:10.728510       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0306 13:42:10.728694       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0306 13:42:10.728730       1 shared_informer.go:273] Waiting for caches to sync for cluster_authentication_trust_controller
I0306 13:42:10.741280       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0306 13:42:10.741315       1 shared_informer.go:273] Waiting for caches to sync for crd-autoregister
I0306 13:42:10.741376       1 shared_informer.go:280] Caches are synced for crd-autoregister
I0306 13:42:10.742162       1 controller.go:85] Starting OpenAPI controller
I0306 13:42:10.743841       1 controller.go:85] Starting OpenAPI V3 controller
I0306 13:42:10.744332       1 naming_controller.go:291] Starting NamingConditionController
I0306 13:42:10.744942       1 establishing_controller.go:76] Starting EstablishingController
I0306 13:42:10.745021       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0306 13:42:10.745163       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0306 13:42:10.745244       1 crd_finalizer.go:266] Starting CRDFinalizer
I0306 13:42:10.853584       1 trace.go:219] Trace[458719799]: "DeltaFIFO Pop Process" ID:kube-scheduler,Depth:12,Reason:slow event handlers blocking the queue (06-Mar-2023 13:42:10.697) (total time: 155ms):
Trace[458719799]: [155.549158ms] [155.549158ms] END
I0306 13:42:11.260243       1 shared_informer.go:280] Caches are synced for configmaps
{"level":"warn","ts":"2023-03-06T13:42:11.267Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc000531180/127.0.0.1:2379","attempt":0,"error":"rpc error: code = Canceled desc = context canceled"}
E0306 13:42:11.268058       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0306 13:42:11.268342       1 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0306 13:42:11.269652       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0306 13:42:11.271340       1 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0306 13:42:11.272889       1 timeout.go:142] post-timeout activity - time-elapsed: 2.392002ms, GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" result: <nil>
I0306 13:42:11.289524       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0306 13:42:11.291920       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0306 13:42:11.293552       1 shared_informer.go:280] Caches are synced for node_authorizer
I0306 13:42:11.341301       1 shared_informer.go:280] Caches are synced for cluster_authentication_trust_controller
I0306 13:42:11.344222       1 cache.go:39] Caches are synced for autoregister controller
I0306 13:42:11.349534       1 apf_controller.go:366] Running API Priority and Fairness config worker
I0306 13:42:11.349696       1 apf_controller.go:369] Running API Priority and Fairness periodic rebalancing process
I0306 13:42:11.350163       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0306 13:42:11.362322       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0306 13:42:11.668375       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0306 13:42:29.364293       1 controller.go:615] quota admission added evaluator for: endpoints
I0306 13:42:31.256995       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0306 13:44:05.775564       1 trace.go:219] Trace[1221619918]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:5eca6c27-b887-4288-b4e7-f225f3d34bf6,client:127.0.0.1,protocol:HTTP/2.0,resource:namespaces,scope:resource,url:/api/v1/namespaces/default,user-agent:kube-apiserver/v1.26.1 (linux/amd64) kubernetes/8f94681,verb:GET (06-Mar-2023 13:44:04.881) (total time: 894ms):
Trace[1221619918]: ---"About to write a response" 893ms (13:44:05.775)
Trace[1221619918]: [894.061767ms] [894.061767ms] END
I0306 13:44:31.589293       1 trace.go:219] Trace[25060774]: "Get" accept:application/json, */*,audit-id:861b73db-08ba-4fc2-8ef6-cab44f64b75c,client:10.0.2.15,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (06-Mar-2023 13:44:30.354) (total time: 1234ms):
Trace[25060774]: ---"About to write a response" 750ms (13:44:31.104)
Trace[25060774]: ---"Writing http response done" 483ms (13:44:31.588)
Trace[25060774]: [1.234047968s] [1.234047968s] END

* 
* ==> kube-apiserver [6d337205f63c] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 13:41:51.648052       1 logging.go:59] [core] [Channel #106 SubChannel #107] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 13:41:51.648282       1 logging.go:59] [core] [Channel #88 SubChannel #89] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 13:41:51.648381       1 logging.go:59] [core] [Channel #70 SubChannel #71] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 13:41:51.648625       1 logging.go:59] [core] [Channel #163 SubChannel #164] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 13:41:51.648676       1 logging.go:59] [core] [Channel #1 SubChannel #2] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 13:41:51.648766       1 logging.go:59] [core] [Channel #55 SubChannel #56] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0306 13:41:51.648880       1 logging.go:59] [core] [Channel #136 SubChannel #137] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-controller-manager [87847107d46c] <==
* I0306 13:42:30.767338       1 controllermanager.go:622] Started "csrcleaner"
I0306 13:42:30.767469       1 cleaner.go:82] Starting CSR cleaner controller
I0306 13:42:30.774442       1 controllermanager.go:622] Started "tokencleaner"
I0306 13:42:30.775464       1 tokencleaner.go:111] Starting token cleaner controller
I0306 13:42:30.776163       1 shared_informer.go:273] Waiting for caches to sync for token_cleaner
I0306 13:42:30.776641       1 shared_informer.go:280] Caches are synced for token_cleaner
I0306 13:42:30.781409       1 controllermanager.go:622] Started "persistentvolume-binder"
I0306 13:42:30.782903       1 pv_controller_base.go:318] Starting persistent volume controller
I0306 13:42:30.782975       1 shared_informer.go:273] Waiting for caches to sync for persistent volume
I0306 13:42:30.795455       1 shared_informer.go:273] Waiting for caches to sync for resource quota
I0306 13:42:30.837022       1 shared_informer.go:280] Caches are synced for TTL after finished
W0306 13:42:30.852537       1 actual_state_of_world.go:541] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube" does not exist
I0306 13:42:30.881369       1 shared_informer.go:280] Caches are synced for expand
I0306 13:42:30.895235       1 shared_informer.go:280] Caches are synced for TTL
I0306 13:42:30.933037       1 shared_informer.go:280] Caches are synced for PV protection
I0306 13:42:30.948399       1 shared_informer.go:280] Caches are synced for ClusterRoleAggregator
I0306 13:42:30.948859       1 shared_informer.go:280] Caches are synced for node
I0306 13:42:30.949173       1 range_allocator.go:167] Sending events to api server.
I0306 13:42:30.949535       1 range_allocator.go:171] Starting range CIDR allocator
I0306 13:42:30.950590       1 shared_informer.go:273] Waiting for caches to sync for cidrallocator
I0306 13:42:30.951558       1 shared_informer.go:280] Caches are synced for cidrallocator
I0306 13:42:30.958763       1 shared_informer.go:280] Caches are synced for cronjob
I0306 13:42:30.967306       1 shared_informer.go:280] Caches are synced for bootstrap_signer
I0306 13:42:30.989951       1 shared_informer.go:280] Caches are synced for crt configmap
I0306 13:42:30.993293       1 shared_informer.go:280] Caches are synced for service account
I0306 13:42:30.999706       1 shared_informer.go:280] Caches are synced for namespace
I0306 13:42:31.000753       1 shared_informer.go:280] Caches are synced for resource quota
I0306 13:42:31.001135       1 shared_informer.go:280] Caches are synced for GC
I0306 13:42:31.004581       1 shared_informer.go:280] Caches are synced for job
I0306 13:42:31.005240       1 shared_informer.go:280] Caches are synced for ReplicationController
I0306 13:42:31.010952       1 shared_informer.go:280] Caches are synced for endpoint
I0306 13:42:31.022608       1 shared_informer.go:280] Caches are synced for daemon sets
I0306 13:42:31.032742       1 shared_informer.go:280] Caches are synced for HPA
I0306 13:42:31.044941       1 shared_informer.go:280] Caches are synced for deployment
I0306 13:42:31.034136       1 shared_informer.go:280] Caches are synced for PVC protection
I0306 13:42:31.038244       1 shared_informer.go:280] Caches are synced for ReplicaSet
I0306 13:42:31.040891       1 shared_informer.go:280] Caches are synced for taint
I0306 13:42:31.053275       1 node_lifecycle_controller.go:1438] Initializing eviction metric for zone: 
W0306 13:42:31.053448       1 node_lifecycle_controller.go:1053] Missing timestamp for Node minikube. Assuming now as a timestamp.
I0306 13:42:31.053605       1 node_lifecycle_controller.go:1254] Controller detected that zone  is now in state Normal.
I0306 13:42:31.041330       1 shared_informer.go:280] Caches are synced for endpoint_slice_mirroring
I0306 13:42:31.056349       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0306 13:42:31.056601       1 taint_manager.go:211] "Sending events to api server"
I0306 13:42:31.065048       1 shared_informer.go:280] Caches are synced for stateful set
I0306 13:42:31.066053       1 shared_informer.go:280] Caches are synced for ephemeral
I0306 13:42:31.041455       1 shared_informer.go:280] Caches are synced for disruption
I0306 13:42:31.069323       1 shared_informer.go:280] Caches are synced for attach detach
I0306 13:42:31.071753       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-kubelet-client
I0306 13:42:31.074793       1 shared_informer.go:280] Caches are synced for resource quota
I0306 13:42:31.075177       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0306 13:42:31.080486       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-kubelet-serving
I0306 13:42:31.076174       1 shared_informer.go:280] Caches are synced for endpoint_slice
I0306 13:42:31.083237       1 shared_informer.go:280] Caches are synced for persistent volume
I0306 13:42:31.085874       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-legacy-unknown
I0306 13:42:31.088799       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0306 13:42:31.086200       1 shared_informer.go:280] Caches are synced for certificate-csrapproving
I0306 13:42:31.155825       1 shared_informer.go:273] Waiting for caches to sync for garbage collector
I0306 13:42:31.451236       1 shared_informer.go:280] Caches are synced for garbage collector
I0306 13:42:31.451320       1 garbagecollector.go:163] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0306 13:42:31.457320       1 shared_informer.go:280] Caches are synced for garbage collector

* 
* ==> kube-controller-manager [a07448956823] <==
* I0306 13:41:43.329522       1 controllermanager.go:622] Started "cronjob"
I0306 13:41:43.329757       1 cronjob_controllerv2.go:137] "Starting cronjob controller v2"
I0306 13:41:43.330881       1 shared_informer.go:273] Waiting for caches to sync for cronjob
I0306 13:41:43.341181       1 controllermanager.go:622] Started "csrapproving"
I0306 13:41:43.341289       1 certificate_controller.go:112] Starting certificate controller "csrapproving"
I0306 13:41:43.342233       1 shared_informer.go:273] Waiting for caches to sync for certificate-csrapproving
I0306 13:41:43.354373       1 controllermanager.go:622] Started "ttl"
I0306 13:41:43.359224       1 ttl_controller.go:120] Starting TTL controller
I0306 13:41:43.359409       1 shared_informer.go:273] Waiting for caches to sync for TTL
I0306 13:41:43.383359       1 shared_informer.go:273] Waiting for caches to sync for resource quota
I0306 13:41:43.518918       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-kubelet-serving
I0306 13:41:43.538172       1 shared_informer.go:280] Caches are synced for PVC protection
I0306 13:41:43.542502       1 shared_informer.go:280] Caches are synced for ReplicaSet
I0306 13:41:43.545144       1 shared_informer.go:280] Caches are synced for HPA
I0306 13:41:43.546347       1 shared_informer.go:280] Caches are synced for certificate-csrapproving
I0306 13:41:43.552575       1 shared_informer.go:280] Caches are synced for expand
I0306 13:41:43.564751       1 shared_informer.go:280] Caches are synced for bootstrap_signer
I0306 13:41:43.569595       1 shared_informer.go:280] Caches are synced for disruption
W0306 13:41:43.570848       1 actual_state_of_world.go:541] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube" does not exist
I0306 13:41:43.572042       1 shared_informer.go:280] Caches are synced for TTL after finished
I0306 13:41:43.579253       1 shared_informer.go:280] Caches are synced for stateful set
I0306 13:41:43.581954       1 shared_informer.go:280] Caches are synced for ephemeral
I0306 13:41:43.584030       1 shared_informer.go:280] Caches are synced for resource quota
I0306 13:41:43.584274       1 shared_informer.go:280] Caches are synced for endpoint
I0306 13:41:43.588442       1 shared_informer.go:280] Caches are synced for job
I0306 13:41:43.588918       1 shared_informer.go:280] Caches are synced for GC
I0306 13:41:43.589569       1 shared_informer.go:280] Caches are synced for ReplicationController
I0306 13:41:43.594420       1 shared_informer.go:280] Caches are synced for ClusterRoleAggregator
I0306 13:41:43.596686       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-kubelet-client
I0306 13:41:43.597123       1 shared_informer.go:280] Caches are synced for node
I0306 13:41:43.597419       1 range_allocator.go:167] Sending events to api server.
I0306 13:41:43.597675       1 range_allocator.go:171] Starting range CIDR allocator
I0306 13:41:43.599265       1 shared_informer.go:273] Waiting for caches to sync for cidrallocator
I0306 13:41:43.598985       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0306 13:41:43.600447       1 shared_informer.go:280] Caches are synced for certificate-csrsigning-legacy-unknown
I0306 13:41:43.599947       1 shared_informer.go:280] Caches are synced for cidrallocator
I0306 13:41:43.604031       1 shared_informer.go:280] Caches are synced for persistent volume
I0306 13:41:43.609100       1 shared_informer.go:280] Caches are synced for deployment
I0306 13:41:43.610456       1 shared_informer.go:280] Caches are synced for attach detach
I0306 13:41:43.621939       1 shared_informer.go:280] Caches are synced for endpoint_slice
I0306 13:41:43.622788       1 shared_informer.go:280] Caches are synced for PV protection
I0306 13:41:43.623038       1 shared_informer.go:280] Caches are synced for endpoint_slice_mirroring
I0306 13:41:43.623064       1 shared_informer.go:280] Caches are synced for namespace
I0306 13:41:43.623943       1 shared_informer.go:280] Caches are synced for service account
I0306 13:41:43.624894       1 shared_informer.go:280] Caches are synced for taint
I0306 13:41:43.625729       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0306 13:41:43.626044       1 taint_manager.go:211] "Sending events to api server"
I0306 13:41:43.626704       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0306 13:41:43.627294       1 shared_informer.go:280] Caches are synced for daemon sets
I0306 13:41:43.627677       1 shared_informer.go:280] Caches are synced for crt configmap
I0306 13:41:43.626564       1 node_lifecycle_controller.go:1438] Initializing eviction metric for zone: 
W0306 13:41:43.630330       1 node_lifecycle_controller.go:1053] Missing timestamp for Node minikube. Assuming now as a timestamp.
I0306 13:41:43.630706       1 node_lifecycle_controller.go:1254] Controller detected that zone  is now in state Normal.
I0306 13:41:43.631077       1 shared_informer.go:280] Caches are synced for cronjob
I0306 13:41:43.650067       1 shared_informer.go:280] Caches are synced for resource quota
I0306 13:41:43.663102       1 shared_informer.go:280] Caches are synced for TTL
I0306 13:41:43.702869       1 shared_informer.go:273] Waiting for caches to sync for garbage collector
I0306 13:41:44.004506       1 shared_informer.go:280] Caches are synced for garbage collector
I0306 13:41:44.056210       1 shared_informer.go:280] Caches are synced for garbage collector
I0306 13:41:44.056319       1 garbagecollector.go:163] Garbage collector: all resource monitors have synced. Proceeding to collect garbage

* 
* ==> kube-proxy [b474fae41d70] <==
* I0306 13:41:29.664519       1 node.go:163] Successfully retrieved node IP: 10.0.2.15
I0306 13:41:29.695070       1 server_others.go:109] "Detected node IP" address="10.0.2.15"
I0306 13:41:29.727930       1 server_others.go:535] "Using iptables proxy"
I0306 13:41:30.219352       1 server_others.go:170] "kube-proxy running in single-stack mode, this ipFamily is not supported" ipFamily=IPv6
I0306 13:41:30.219478       1 server_others.go:176] "Using iptables Proxier"
I0306 13:41:30.219554       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0306 13:41:30.222185       1 server.go:655] "Version info" version="v1.26.1"
I0306 13:41:30.222468       1 server.go:657] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0306 13:41:30.224189       1 config.go:317] "Starting service config controller"
I0306 13:41:30.225149       1 shared_informer.go:273] Waiting for caches to sync for service config
I0306 13:41:30.225460       1 config.go:226] "Starting endpoint slice config controller"
I0306 13:41:30.226510       1 shared_informer.go:273] Waiting for caches to sync for endpoint slice config
I0306 13:41:30.233702       1 config.go:444] "Starting node config controller"
I0306 13:41:30.234404       1 shared_informer.go:273] Waiting for caches to sync for node config
I0306 13:41:30.338266       1 shared_informer.go:280] Caches are synced for node config
I0306 13:41:30.338783       1 shared_informer.go:280] Caches are synced for endpoint slice config
I0306 13:41:30.394876       1 shared_informer.go:280] Caches are synced for service config

* 
* ==> kube-proxy [d3e945555518] <==
* I0306 13:42:15.367656       1 node.go:163] Successfully retrieved node IP: 10.0.2.15
I0306 13:42:15.367831       1 server_others.go:109] "Detected node IP" address="10.0.2.15"
I0306 13:42:15.367889       1 server_others.go:535] "Using iptables proxy"
I0306 13:42:15.450075       1 server_others.go:170] "kube-proxy running in single-stack mode, this ipFamily is not supported" ipFamily=IPv6
I0306 13:42:15.450171       1 server_others.go:176] "Using iptables Proxier"
I0306 13:42:15.450247       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0306 13:42:15.451838       1 server.go:655] "Version info" version="v1.26.1"
I0306 13:42:15.451919       1 server.go:657] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0306 13:42:15.453144       1 config.go:317] "Starting service config controller"
I0306 13:42:15.453260       1 shared_informer.go:273] Waiting for caches to sync for service config
I0306 13:42:15.453332       1 config.go:226] "Starting endpoint slice config controller"
I0306 13:42:15.453367       1 shared_informer.go:273] Waiting for caches to sync for endpoint slice config
I0306 13:42:15.461960       1 config.go:444] "Starting node config controller"
I0306 13:42:15.461987       1 shared_informer.go:273] Waiting for caches to sync for node config
I0306 13:42:15.555768       1 shared_informer.go:280] Caches are synced for service config
I0306 13:42:15.556441       1 shared_informer.go:280] Caches are synced for endpoint slice config
I0306 13:42:15.562129       1 shared_informer.go:280] Caches are synced for node config

* 
* ==> kube-scheduler [0abe95ef90b8] <==
* I0306 13:41:23.906654       1 serving.go:348] Generated self-signed cert in-memory
W0306 13:41:29.040544       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0306 13:41:29.041483       1 authentication.go:349] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0306 13:41:29.041722       1 authentication.go:350] Continuing without authentication configuration. This may treat all requests as anonymous.
W0306 13:41:29.041843       1 authentication.go:351] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0306 13:41:29.205263       1 server.go:152] "Starting Kubernetes Scheduler" version="v1.26.1"
I0306 13:41:29.205367       1 server.go:154] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0306 13:41:29.215837       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0306 13:41:29.216536       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0306 13:41:29.218524       1 shared_informer.go:273] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0306 13:41:29.216590       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0306 13:41:29.272091       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0306 13:41:29.272617       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0306 13:41:29.305055       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0306 13:41:29.305315       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0306 13:41:29.305691       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0306 13:41:29.306099       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0306 13:41:29.306366       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0306 13:41:29.306427       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0306 13:41:29.306714       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0306 13:41:29.306835       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0306 13:41:29.353636       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0306 13:41:29.357744       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0306 13:41:29.358454       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0306 13:41:29.366235       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0306 13:41:29.357638       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0306 13:41:29.366948       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0306 13:41:29.355368       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0306 13:41:29.369138       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0306 13:41:29.373730       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0306 13:41:29.378272       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0306 13:41:29.378814       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0306 13:41:29.378971       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0306 13:41:29.379188       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0306 13:41:29.379248       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0306 13:41:29.390328       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found]
E0306 13:41:29.390978       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found]
W0306 13:41:29.390467       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0306 13:41:29.391578       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
I0306 13:41:30.620103       1 shared_informer.go:280] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
E0306 13:41:50.318758       1 scheduling_queue.go:1065] "Error while retrieving next pod from scheduling queue" err="scheduling queue is closed"
E0306 13:41:50.319038       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kube-scheduler [623dc1b8e1aa] <==
* I0306 13:42:06.974809       1 serving.go:348] Generated self-signed cert in-memory
W0306 13:42:10.657862       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0306 13:42:10.658085       1 authentication.go:349] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0306 13:42:10.658184       1 authentication.go:350] Continuing without authentication configuration. This may treat all requests as anonymous.
W0306 13:42:10.658270       1 authentication.go:351] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0306 13:42:10.758215       1 server.go:152] "Starting Kubernetes Scheduler" version="v1.26.1"
I0306 13:42:10.758597       1 server.go:154] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0306 13:42:10.808699       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0306 13:42:10.809357       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0306 13:42:10.815132       1 shared_informer.go:273] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0306 13:42:10.815276       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0306 13:42:10.980938       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0306 13:42:10.985061       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0306 13:42:10.988236       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0306 13:42:10.998567       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0306 13:42:11.150792       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0306 13:42:11.151618       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0306 13:42:11.178877       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0306 13:42:11.179362       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0306 13:42:11.180292       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0306 13:42:11.182917       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0306 13:42:11.194307       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0306 13:42:11.194831       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0306 13:42:11.195300       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0306 13:42:11.195940       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0306 13:42:11.200846       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0306 13:42:11.201406       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0306 13:42:11.208026       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0306 13:42:11.208472       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0306 13:42:11.212734       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0306 13:42:11.213148       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0306 13:42:11.216604       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0306 13:42:11.217154       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0306 13:42:11.230843       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0306 13:42:11.234266       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0306 13:42:11.240272       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0306 13:42:11.241101       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0306 13:42:11.249475       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0306 13:42:11.250321       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0306 13:42:11.252193       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0306 13:42:11.255473       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
I0306 13:42:12.815683       1 shared_informer.go:280] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* -- Journal begins at Mon 2023-03-06 09:13:55 UTC, ends at Mon 2023-03-06 13:46:13 UTC. --
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.255311  195990 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/ea56a38e43622561d394bd631aeb3522-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"ea56a38e43622561d394bd631aeb3522\") " pod="kube-system/kube-apiserver-minikube"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.255629  195990 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/ea56a38e43622561d394bd631aeb3522-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"ea56a38e43622561d394bd631aeb3522\") " pod="kube-system/kube-apiserver-minikube"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.255735  195990 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/ea56a38e43622561d394bd631aeb3522-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"ea56a38e43622561d394bd631aeb3522\") " pod="kube-system/kube-apiserver-minikube"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.342871  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="61c52f61d76a8071e17437413ea8c5b7d96a2dcdf6459de5414d0e516d06dfcb"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.343346  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="761f91c58a3d25c4cffc737f92473f7d4e89fa4b2c3dc7a47f4dd67b30cde4cc"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.343996  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="def938aa698e5646879583fcee75b1316dcf2fd07420cec9ecd1ac877c8a79c7"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.345298  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="5141b13d4a47cbe376337dd03e5c800a022360c691452fad3ac0b2bbe9679cb8"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.345594  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3ffc98eeda1bd2adf3515fe3fef4d1b9aefbe3952ec800c1b7f09311161deb39"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.349871  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3c0a328b939f95290e956022262be658f7d347f88b16a06b41a7d3f39b009c66"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.353945  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="b877d3c7f5036c0cd97b741a8d672bdad71dbce4eb17d87a821cb7b1632244e4"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.355101  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="b03f99142d433b8fba5089de76bde719846fd489ea7265489186c9f4982ca0b9"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.358762  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="745ace00680203ad1c9a59601130dd48950db6d957e8438195c6875270ce96ec"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.360063  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="0761582fbdb44ec50fca1f0fe9baec95bf895b50cdf0ba1443deb7dc712406c2"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.360470  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="0505f646fd0b22f051277eec8d0cd62af28c01c8d7d83334216f218e035cfd75"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.360658  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e01796093a91f849d5135114f8589c0ab7d2ebe2de761a6ce62e51f948280dfc"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.360850  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="a7fb7f92f805ff5033b5bbc92526f48790072a439edb88514005c43069745364"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.361349  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="4b8921c13a3bdbf3f0c0c8577a9cc5174bc158117dd9bcdb61d0f76975451ed0"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.367250  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="611d25f69dc490d6de775fc1941ea898cde1bf2e72e7b5611c44f3a487576817"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.385291  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="420c7c4f3ddc8f7998bf311e5c5f349208d11d999765bb0ed971d0905895ccf8"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.385738  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="bf7dd107c4d992974fe77c8ebd0add21df2e7777e286e72a2def37f2b3b9c9f5"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.386228  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="704d802b1f2f838eeb011a5a79e11cfd89b540022074a9527d309a4c8a2d22bd"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.386364  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="75f25f248373a8edae412def1b56737ee640505c124833ace55d9dc9046bfb36"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.386717  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="496ee56836928ad434c2ddb5fa0648c091f870c86f477819ddadf85d1d7a3205"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.386834  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="f35b01efc56651de07b8ce78006b7f769e8781adf863e943911a64195b49260b"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.389043  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="ad151dd8150ff091e529dd2d68bf9cf33a90f05944f5d309e81d7860deed9f13"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.389580  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="442678e1514e429242733015be861e3b5e8553d1a301abe988238bf164f9e9da"
Mar 06 13:42:02 minikube kubelet[195990]: I0306 13:42:02.416114  195990 scope.go:115] "RemoveContainer" containerID="397454e5053f062549193ce6c2d7d6d945174137bca7345b7c0b83c751a137a6"
Mar 06 13:42:03 minikube kubelet[195990]: I0306 13:42:03.299241  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="615d83ffd96d7cf8fcd7c1a1cf7f795e3be618c840c7b6e2435f77a6d2055855"
Mar 06 13:42:11 minikube kubelet[195990]: E0306 13:42:11.232707  195990 controller.go:146] failed to ensure lease exists, will retry in 200ms, error: Get "https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s": net/http: request canceled (Client.Timeout exceeded while awaiting headers)
Mar 06 13:42:11 minikube kubelet[195990]: I0306 13:42:11.264186  195990 trace.go:219] Trace[223094020]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (06-Mar-2023 13:42:01.138) (total time: 10126ms):
Mar 06 13:42:11 minikube kubelet[195990]: Trace[223094020]: ---"Objects listed" error:<nil> 10125ms (13:42:11.263)
Mar 06 13:42:11 minikube kubelet[195990]: Trace[223094020]: [10.126055942s] [10.126055942s] END
Mar 06 13:42:11 minikube kubelet[195990]: I0306 13:42:11.273079  195990 trace.go:219] Trace[1397993942]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (06-Mar-2023 13:42:01.143) (total time: 10129ms):
Mar 06 13:42:11 minikube kubelet[195990]: Trace[1397993942]: ---"Objects listed" error:<nil> 10129ms (13:42:11.272)
Mar 06 13:42:11 minikube kubelet[195990]: Trace[1397993942]: [10.129630947s] [10.129630947s] END
Mar 06 13:42:11 minikube kubelet[195990]: I0306 13:42:11.471593  195990 kubelet_node_status.go:108] "Node was previously registered" node="minikube"
Mar 06 13:42:11 minikube kubelet[195990]: I0306 13:42:11.472177  195990 kubelet_node_status.go:73] "Successfully registered node" node="minikube"
Mar 06 13:42:11 minikube kubelet[195990]: I0306 13:42:11.482722  195990 kuberuntime_manager.go:1114] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Mar 06 13:42:11 minikube kubelet[195990]: I0306 13:42:11.486721  195990 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Mar 06 13:42:12 minikube kubelet[195990]: I0306 13:42:12.162271  195990 apiserver.go:52] "Watching apiserver"
Mar 06 13:42:12 minikube kubelet[195990]: I0306 13:42:12.207459  195990 topology_manager.go:210] "Topology Admit Handler"
Mar 06 13:42:12 minikube kubelet[195990]: I0306 13:42:12.207626  195990 topology_manager.go:210] "Topology Admit Handler"
Mar 06 13:42:12 minikube kubelet[195990]: I0306 13:42:12.207759  195990 topology_manager.go:210] "Topology Admit Handler"
Mar 06 13:42:12 minikube kubelet[195990]: I0306 13:42:12.207913  195990 topology_manager.go:210] "Topology Admit Handler"
Mar 06 13:42:12 minikube kubelet[195990]: I0306 13:42:12.248474  195990 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
Mar 06 13:42:12 minikube kubelet[195990]: I0306 13:42:12.277280  195990 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/e5cb6f62-2ea6-4c8b-8c9d-b95681a87bb4-tmp\") pod \"storage-provisioner\" (UID: \"e5cb6f62-2ea6-4c8b-8c9d-b95681a87bb4\") " pod="kube-system/storage-provisioner"
Mar 06 13:42:12 minikube kubelet[195990]: I0306 13:42:12.279124  195990 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/bf5ece6b-a815-499c-bfa6-043293e17306-xtables-lock\") pod \"kube-proxy-wtqlm\" (UID: \"bf5ece6b-a815-499c-bfa6-043293e17306\") " pod="kube-system/kube-proxy-wtqlm"
Mar 06 13:42:12 minikube kubelet[195990]: I0306 13:42:12.280397  195990 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-87dfr\" (UniqueName: \"kubernetes.io/projected/bf5ece6b-a815-499c-bfa6-043293e17306-kube-api-access-87dfr\") pod \"kube-proxy-wtqlm\" (UID: \"bf5ece6b-a815-499c-bfa6-043293e17306\") " pod="kube-system/kube-proxy-wtqlm"
Mar 06 13:42:12 minikube kubelet[195990]: I0306 13:42:12.280705  195990 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/61ae3149-e1e4-4b07-a1ac-750750531ddb-config-volume\") pod \"coredns-787d4945fb-9jkj4\" (UID: \"61ae3149-e1e4-4b07-a1ac-750750531ddb\") " pod="kube-system/coredns-787d4945fb-9jkj4"
Mar 06 13:42:12 minikube kubelet[195990]: I0306 13:42:12.280997  195990 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-wb7rw\" (UniqueName: \"kubernetes.io/projected/61ae3149-e1e4-4b07-a1ac-750750531ddb-kube-api-access-wb7rw\") pod \"coredns-787d4945fb-9jkj4\" (UID: \"61ae3149-e1e4-4b07-a1ac-750750531ddb\") " pod="kube-system/coredns-787d4945fb-9jkj4"
Mar 06 13:42:12 minikube kubelet[195990]: I0306 13:42:12.281489  195990 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-sh7fn\" (UniqueName: \"kubernetes.io/projected/fef45a5c-e7bb-477a-94c1-7ccd2b5485eb-kube-api-access-sh7fn\") pod \"coredns-787d4945fb-f2hdp\" (UID: \"fef45a5c-e7bb-477a-94c1-7ccd2b5485eb\") " pod="kube-system/coredns-787d4945fb-f2hdp"
Mar 06 13:42:12 minikube kubelet[195990]: I0306 13:42:12.288760  195990 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-tbtps\" (UniqueName: \"kubernetes.io/projected/e5cb6f62-2ea6-4c8b-8c9d-b95681a87bb4-kube-api-access-tbtps\") pod \"storage-provisioner\" (UID: \"e5cb6f62-2ea6-4c8b-8c9d-b95681a87bb4\") " pod="kube-system/storage-provisioner"
Mar 06 13:42:12 minikube kubelet[195990]: I0306 13:42:12.289083  195990 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/fef45a5c-e7bb-477a-94c1-7ccd2b5485eb-config-volume\") pod \"coredns-787d4945fb-f2hdp\" (UID: \"fef45a5c-e7bb-477a-94c1-7ccd2b5485eb\") " pod="kube-system/coredns-787d4945fb-f2hdp"
Mar 06 13:42:12 minikube kubelet[195990]: I0306 13:42:12.289359  195990 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/bf5ece6b-a815-499c-bfa6-043293e17306-kube-proxy\") pod \"kube-proxy-wtqlm\" (UID: \"bf5ece6b-a815-499c-bfa6-043293e17306\") " pod="kube-system/kube-proxy-wtqlm"
Mar 06 13:42:12 minikube kubelet[195990]: I0306 13:42:12.289762  195990 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/bf5ece6b-a815-499c-bfa6-043293e17306-lib-modules\") pod \"kube-proxy-wtqlm\" (UID: \"bf5ece6b-a815-499c-bfa6-043293e17306\") " pod="kube-system/kube-proxy-wtqlm"
Mar 06 13:42:12 minikube kubelet[195990]: I0306 13:42:12.291035  195990 reconciler.go:41] "Reconciler: start to sync state"
Mar 06 13:42:12 minikube kubelet[195990]: I0306 13:42:12.543083  195990 scope.go:115] "RemoveContainer" containerID="bba02e864b5e21e49ee0b21e749fe36019f4055bfda3c1a10f24195103a1d1c9"
Mar 06 13:42:13 minikube kubelet[195990]: I0306 13:42:13.155201  195990 scope.go:115] "RemoveContainer" containerID="b474fae41d70434a76e807c2ec01b1e8e3d5dd23c58d308e3bd6c9675153513a"
Mar 06 13:42:13 minikube kubelet[195990]: I0306 13:42:13.164965  195990 scope.go:115] "RemoveContainer" containerID="1df922a0056b7d143ff3fb725d67de148640b85212fd2249c3ff6a52601106c0"
Mar 06 13:42:19 minikube kubelet[195990]: I0306 13:42:19.312839  195990 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="10942a691b257f6f2dc8bd751e8c0752db53ecf57e7f5087a6345edeedcfb8a8"

* 
* ==> storage-provisioner [7fb42f6d4909] <==
* I0306 13:42:14.038620       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0306 13:42:14.106332       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0306 13:42:14.106395       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0306 13:42:29.371541       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0306 13:42:29.373134       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_47e4e1b6-c52d-4f05-8d5b-d688268c8e50!
I0306 13:42:29.373873       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"bf1d2c9b-4174-4b40-8974-c2feb91dd293", APIVersion:"v1", ResourceVersion:"10932", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_47e4e1b6-c52d-4f05-8d5b-d688268c8e50 became leader
I0306 13:42:29.474070       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_47e4e1b6-c52d-4f05-8d5b-d688268c8e50!

* 
* ==> storage-provisioner [bba02e864b5e] <==
* I0306 13:41:24.002184       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0306 13:41:29.660165       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0306 13:41:29.660435       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0306 13:41:47.215802       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0306 13:41:47.222609       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"bf1d2c9b-4174-4b40-8974-c2feb91dd293", APIVersion:"v1", ResourceVersion:"10842", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_e17a7db9-1edf-481e-b3f2-a90f987e3931 became leader
I0306 13:41:47.223845       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_e17a7db9-1edf-481e-b3f2-a90f987e3931!
I0306 13:41:47.424574       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_e17a7db9-1edf-481e-b3f2-a90f987e3931!

